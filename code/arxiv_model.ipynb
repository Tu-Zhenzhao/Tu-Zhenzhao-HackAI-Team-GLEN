{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b4b3df-2b30-49c4-9d82-a2787aa467d3",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Language Model with LoRA and Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c524c9-10a9-4b56-97f8-f43e57f73838",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project demonstrates how to fine-tune a pretrained Large Language Model (LLM) using LoRA (Low-Rank Adaptation) on a custom dataset. The model is configured to learn from research paper titles and summaries, where it generates assistant-like responses based on user queries. We’ll walk through the steps, from setting up the environment to loading data, formatting, tokenizing, and training the model.\n",
    "\n",
    "### Prerequisites\n",
    "- Basic understanding of Python and deep learning concepts.\n",
    "- GPU-enabled machine with CUDA support (NVIDIA RTX 4090 is used in this project).\n",
    "- Libraries required:\n",
    "    - PyTorch\n",
    "    - Transformers\n",
    "    - PEFT (Parameter-Efficient Fine-Tuning)\n",
    "    - pandas\n",
    "    - torch\n",
    "\n",
    "#### 1. Setting Up Your Environment\n",
    "\n",
    "Ensure your machine supports CUDA and PyTorch is installed with GPU support. We’ll first check if the GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337d780-b262-490c-87de-e3c735fc0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91683bdf-38d5-49da-b942-9b04d716b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test see if GPU is ready\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is ready!\")\n",
    "        device = torch.cuda.get_device_name(0)\n",
    "        print(f\"{device} is ready!\")\n",
    "    else:\n",
    "        print(\"CUDA is gone...\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd5f24e-ad03-446e-99c3-15aaa5b18b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is ready!\n",
      "NVIDIA GeForce RTX 4090 is ready!\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a99a76-aae3-49e4-8856-40f633d231ef",
   "metadata": {},
   "source": [
    "#### 2. Model and Data Preparation\n",
    "\n",
    "We begin by specifying the base model and loading a dataset (in this case, research papers with titles and summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea23b71-0eea-4c58-a1e0-f765eefa49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model info\n",
    "base_model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "new_model = \"/project/models/NV-llama3.1-8b-Arxiv\"\n",
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91405671-0602-4c0c-9ad1-a43b63a3aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv(\"ml_papers.csv\")\n",
    "data = data.dropna(subset=['title', 'summary']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48abc48-ec3c-49ae-84ed-e5b710a17f8b",
   "metadata": {},
   "source": [
    "#### 3. Extracting Topics and Generating Queries\n",
    "\n",
    "To fine-tune the model for specific user queries, we extract the topics from the paper titles and generate appropriate user queries and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd07d0cc-1385-42ae-b589-b444bcaa5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topics from titles\n",
    "def extract_topic(title):\n",
    "    title = re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", title)\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = title.lower()\n",
    "    return title.strip()\n",
    "\n",
    "# Generate user queries\n",
    "def generate_user_query(topic):\n",
    "    return f\"I'm looking for papers discussing {topic}.\"\n",
    "\n",
    "# Create assistant responses\n",
    "def create_assistant_response(row):\n",
    "    title = row['title']\n",
    "    summary = row['summary']\n",
    "    response = f\"One paper that discusses this topic is '{title}'. {summary}\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b330689-5b02-480d-a663-f22a40265a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic'] = data['title'].apply(extract_topic)\n",
    "data['instruction'] = data['topic'].apply(generate_user_query)\n",
    "\n",
    "# generate assistant responses\n",
    "data['response'] = data.apply(create_assistant_response, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d71b4d-08cf-41ab-9814-0ba8bb31cc70",
   "metadata": {},
   "source": [
    "#### 4. Formatting the Dataset for the Model\n",
    "\n",
    "We define custom tokens for formatting our data into the structure that the model can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3819d9c8-11f4-4739-9627-4a8c9dba575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "bos_token = \"<bos>\"\n",
    "eos_token = \"<eos>\"\n",
    "user_start = \"<user>\"\n",
    "user_end = \"</user>\"\n",
    "assistant_start = \"<assistant>\"\n",
    "assistant_end = \"</assistant>\"\n",
    "pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8083419-59e8-4699-803d-3f05088cd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format examples\n",
    "def format_example(instruction, response):\n",
    "    return f\"{bos_token}\\n{user_start}\\n{instruction}\\n{user_end}\\n{assistant_start}\\n{response}\\n{assistant_end}\\n{eos_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c4f817f-9363-4af7-b3ef-da5f3b722316",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: format_example(row['instruction'], row['response']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd394163-331d-46d6-83f0-e490f4d45a1e",
   "metadata": {},
   "source": [
    "#### 5. Defining the Custom Dataset Class\n",
    "\n",
    "We create a custom Dataset class to handle tokenized inputs, attention masks, and labels. The labels are set to -100 for tokens that are not relevant to the assistant’s response, which ensures the model only learns from the assistant’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ffe2dde-483f-40b3-8748-dd15e467aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "class PapersDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "        self.labels = tokenized_data['input_ids'].clone()\n",
    "\n",
    "        assistant_start_id = tokenizer.convert_tokens_to_ids(assistant_start)\n",
    "        assistant_end_id = tokenizer.convert_tokens_to_ids(assistant_end)\n",
    "\n",
    "        for i in range(len(self.labels)):\n",
    "            input_ids = self.input_ids[i]\n",
    "            labels = self.labels[i]\n",
    "\n",
    "            assistant_start_positions = (input_ids == assistant_start_id).nonzero(as_tuple=True)[0]\n",
    "            assistant_end_positions = (input_ids == assistant_end_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if len(assistant_start_positions) > 0 and len(assistant_end_positions) > 0:\n",
    "                assistant_start_pos = assistant_start_positions[0]\n",
    "                assistant_end_pos = assistant_end_positions[0]\n",
    "\n",
    "                labels[:assistant_start_pos + 1] = -100\n",
    "                labels[assistant_end_pos:] = -100\n",
    "            else:\n",
    "                labels[:] = -100\n",
    "\n",
    "            self.labels[i] = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ccec9d-d494-419d-8a99-2c9ff574fa20",
   "metadata": {},
   "source": [
    "#### 6. Dataset Tokenization\n",
    "\n",
    "Next, we load the tokenizer and tokenize the dataset. The tokenizer is essential for converting the text into token IDs that the model can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a87dfa80-7651-43aa-b8c6-bf2056960fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the BitsAndBytesConfig for 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load model in 8-bit precision\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6bf600-4c91-4c3c-b418-693d033f20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define your special tokens\n",
    "bos_token = \"<bos>\"\n",
    "eos_token = \"<eos>\"\n",
    "pad_token = \"<pad>\"\n",
    "user_start = \"<user>\"\n",
    "user_end = \"</user>\"\n",
    "assistant_start = \"<assistant>\"\n",
    "assistant_end = \"</assistant>\"\n",
    "\n",
    "special_tokens = {\n",
    "    'bos_token': bos_token,\n",
    "    'eos_token': eos_token,\n",
    "    'pad_token': pad_token,\n",
    "    'additional_special_tokens': [user_start, user_end, assistant_start, assistant_end]\n",
    "}\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f37a1bb0-f4c1-4604-afe1-73d86f11cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "tokenized_data = tokenizer(\n",
    "    data['text'].tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84df437-c4e2-4729-9f6b-e64c82a7eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PapersDataset(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e2278-1eda-4ed5-996f-57c4b0fe09c3",
   "metadata": {},
   "source": [
    "#### 7. Model Fine-Tuning with LoRA\n",
    "\n",
    "We apply LoRA to the pretrained model for efficient fine-tuning, loading the model in 4-bit precision for memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1052cfb9-484b-4ebc-a9a8-a8e4d6546f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ba32eb54d847be946a719a70a4761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is important correct one\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/project/models\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Update model's embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Ensure these modules exist in your model\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb91bf-4208-4980-a816-e2286e75b35c",
   "metadata": {},
   "source": [
    "#### 8. Training the Model\n",
    "\n",
    "We use the Trainer from the Hugging Face transformers library to fine-tune the model. The following code sets up training arguments and trains the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f64886-7584-4896-988c-e7e22e15b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments setting for 1 RTX 4090\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"/project/models/NV-arxiv-llama3.1\",             # Where to save results\n",
    "    num_train_epochs=3,                 # Number of epochs\n",
    "    per_device_train_batch_size=2,      # Start with 2, adjust based on memory\n",
    "    gradient_accumulation_steps=5,      # Accumulate gradients to simulate larger batch size\n",
    "    fp16=True,                         # Use FP16 for memory efficiency on RTX 4090\n",
    "    gradient_checkpointing=True,        # Enable gradient checkpointing to save memory\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2e-5,                 # Adjust learning rate for fine-tuning\n",
    "    max_grad_norm=0.3,                  # Gradient clipping\n",
    "    weight_decay=0.001,                 # Regularization\n",
    "    optim=\"adamw_torch\",                      # Use standard AdamW optimizer\n",
    "    max_steps=50,                      # Train for 500 steps\n",
    "    warmup_ratio=0.03,                  # Warmup learning rate\n",
    "    group_by_length=True,               # Group sequences of similar lengths to save memory\n",
    "    save_steps=100,                     # Save model checkpoint every 100 steps\n",
    "    logging_steps=5,                    # Log training progress every 5 steps\n",
    "    report_to=\"none\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a84c7f9-d54d-42e2-a097-2dd75ae2e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2b4502-35ba-4f33-9303-635a9024b420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 02:38, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.120500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ef99c6-0583c59a37cac7677e9626cb;ec601d43-8e34-41db-8d40-a1fddb87114e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.0940171241760255, metrics={'train_runtime': 162.3597, 'train_samples_per_second': 3.08, 'train_steps_per_second': 0.308, 'total_flos': 1.1548546301952e+16, 'train_loss': 2.0940171241760255, 'epoch': 6.25})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a77e4-fcae-4560-9782-8b0f9d122c35",
   "metadata": {},
   "source": [
    "#### 9. Saving and Loading the Model\n",
    "\n",
    "Once training is complete, save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de57ae4b-6b8b-4779-9447-8366f07526d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ef99c7-61f7a8b4340c76d12c012656;60442a9b-6c3d-4836-b56d-1821d66b1378)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the LoRA adapter weights\n",
    "model.save_pretrained(\"/project/models/arxiv_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a51a31-29cc-496b-a5fe-b14335fabb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0e7d06d974d7c8277684faa3c5285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128263, 4096)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key\n",
    ")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/project/models\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Update model's embeddings to accommodate new tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "086a5718-3b60-4561-a460-efaca5bb1828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128263, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128263, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the LoRA adapter weights\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/project/models/arxiv_model\",\n",
    "    device_map=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2d10f-b855-46aa-80b2-3cbc642e5614",
   "metadata": {},
   "source": [
    "#### 10. Generating Text\n",
    "\n",
    "Finally, we generate text from the fine-tuned model using a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95878405-0737-40a1-9812-a42f3197c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the format_example function\n",
    "def format_example(instruction, response=\"\"):\n",
    "    return f\"{bos_token}\\n{user_start}\\n{instruction}\\n{user_end}\\n{assistant_start}\\n{response}\"\n",
    "\n",
    "# Prepare the input\n",
    "instruction = \"I am looking for a paper discussing To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning\"\n",
    "input_text = format_example(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3398e05-e90a-4512-a566-c247b5b89dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f02284ee-21be-44c5-a357-e2f7d70c4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><bos>\n",
      "<user>\n",
      "I am looking for a paper discussing To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning\n",
      "</user>\n",
      "<assistant>\n",
      "This research is focused on the topic of \"Chain-of-Thought\" (CoT) in artificial intelligence. Specifically, it explores how this technique can assist with mathematical problems and symbolic reasoning tasks.\n",
      "\n",
      "The researchers employed both qualitative and quantitative methods to investigate the effectiveness of chain-of-thought explanations across various AI models trained using different architectures such as LSTMs, Transformers, and CNNs. They found that these systems were able to generate helpful chains of thought during problem-solving processes but did so without leveraging human language generation capabilities.\n",
      " \n",
      "To evaluate their results they designed experiments where subjects engaged in solving either real-world math questions presented by an intelligent tutoring system's interface or reading comprehension passages from which one question was posed afterward; then asked them whether each step within any provided sequence would aid someone who wanted help finding solutions themselves rather than relying solely upon pre-existing knowledge alone!\n",
      " \n",
      "\n",
      "Here are some key findings:\n",
      "*   **Effectiveness**: While chain-of-thought does indeed provide benefits when working through challenging material especially concerning numerical data analysis procedures - its overall value proved more pronounced within contexts reliant heavily upon symbolic processing like logic puzzles etc.,\n",
      "*   **Human Language Generation**: Despite having shown significant improvement over other forms assistance offered directly prior our tests also revealed considerable room potential growth even if primarily being utilized towards\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(eos_token),\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    )\n",
    "\n",
    "# Decode and extract the assistant's response\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28992c7-daa8-4518-b642-2f49dfb39d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385374bb-7960-465d-8c8f-47d9b535bcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b145bb6-27cf-4a86-80ff-4d612fd90cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fafb20-1f52-4923-82bd-9de86e8e17e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d26c0f-bd60-465f-b575-f9b47572a8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
