title,summary,pdf_url,arxiv_link
Gender Representation and Bias in Indian Civil Service Mock Interviews,"This paper makes three key contributions. First, via a substantial corpus of
51,278 interview questions sourced from 888 YouTube videos of mock interviews
of Indian civil service candidates, we demonstrate stark gender bias in the
broad nature of questions asked to male and female candidates. Second, our
experiments with large language models show a strong presence of gender bias in
explanations provided by the LLMs on the gender inference task. Finally, we
present a novel dataset of 51,278 interview questions that can inform future
social science studies.",http://arxiv.org/pdf/2409.12194v2,http://arxiv.org/abs/2409.12194v2
DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control,"Imitation learning has proven to be a powerful tool for training complex
visuomotor policies. However, current methods often require hundreds to
thousands of expert demonstrations to handle high-dimensional visual
observations. A key reason for this poor data efficiency is that visual
representations are predominantly either pretrained on out-of-domain data or
trained directly through a behavior cloning objective. In this work, we present
DynaMo, a new in-domain, self-supervised method for learning visual
representations. Given a set of expert demonstrations, we jointly learn a
latent inverse dynamics model and a forward dynamics model over a sequence of
image embeddings, predicting the next frame in latent space, without
augmentations, contrastive sampling, or access to ground truth actions.
Importantly, DynaMo does not require any out-of-domain data such as Internet
datasets or cross-embodied datasets. On a suite of six simulated and real
environments, we show that representations learned with DynaMo significantly
improve downstream imitation learning performance over prior self-supervised
learning objectives, and pretrained representations. Gains from using DynaMo
hold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,
and nearest neighbors. Finally, we ablate over key components of DynaMo and
measure its impact on downstream policy performance. Robot videos are best
viewed at https://dynamo-ssl.github.io",http://arxiv.org/pdf/2409.12192v1,http://arxiv.org/abs/2409.12192v1
Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL
models that redefines the conventional predetermined-resolution approach in
visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism,
which enables the model to dynamically process images of varying resolutions
into different numbers of visual tokens. This approach allows the model to
generate more efficient and accurate visual representations, closely aligning
with human perceptual processes. The model also integrates Multimodal Rotary
Position Embedding (M-RoPE), facilitating the effective fusion of positional
information across text, images, and videos. We employ a unified paradigm for
processing both images and videos, enhancing the model's visual perception
capabilities. To explore the potential of large multimodal models, Qwen2-VL
investigates the scaling laws for large vision-language models (LVLMs). By
scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the
amount of training data, the Qwen2-VL Series achieves highly competitive
performance. Notably, the Qwen2-VL-72B model achieves results comparable to
leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal
benchmarks, outperforming other generalist models. Code is available at
\url{https://github.com/QwenLM/Qwen2-VL}.",http://arxiv.org/pdf/2409.12191v1,http://arxiv.org/abs/2409.12191v1
Massively Multi-Person 3D Human Motion Forecasting with Scene Context,"Forecasting long-term 3D human motion is challenging: the stochasticity of
human behavior makes it hard to generate realistic human motion from the input
sequence alone. Information on the scene environment and the motion of nearby
people can greatly aid the generation process. We propose a scene-aware social
transformer model (SAST) to forecast long-term (10s) human motion motion.
Unlike previous models, our approach can model interactions between both widely
varying numbers of people and objects in a scene. We combine a temporal
convolutional encoder-decoder architecture with a Transformer-based bottleneck
that allows us to efficiently combine motion and scene information. We model
the conditional motion distribution using denoising diffusion models. We
benchmark our approach on the Humans in Kitchens dataset, which contains 1 to
16 persons and 29 to 50 objects that are visible simultaneously. Our model
outperforms other approaches in terms of realism and diversity on different
metrics and in a user study. Code is available at
https://github.com/felixbmuller/SAST.",http://arxiv.org/pdf/2409.12189v1,http://arxiv.org/abs/2409.12189v1
SPECTER: An Instrument Concept for CMB Spectral Distortion Measurements with Enhanced Sensitivity,"Deviations of the cosmic microwave background (CMB) energy spectrum from a
perfect blackbody uniquely probe a wide range of physics, ranging from
fundamental physics in the primordial Universe ($\mu$-distortion) to late-time
baryonic feedback processes (y-distortion). While the y-distortion can be
detected with a moderate increase in sensitivity over that of COBE/FIRAS, the
$\Lambda$CDM-predicted $\mu$-distortion is roughly two orders of magnitude
smaller and requires substantial improvements, with foregrounds presenting a
serious obstacle. Within the standard model, the dominant contribution to $\mu$
arises from energy injected via Silk damping, yielding sensitivity to the
primordial power spectrum at wavenumbers $k \approx 1-10^{4}$ Mpc$^{-1}$. Here,
we present a new instrument concept, SPECTER, with the goal of robustly
detecting $\mu$. The instrument technology is similar to that of LiteBIRD, but
with an absolute temperature calibration system. Using a Fisher approach, we
optimize the instrument's configuration to target $\mu$ while robustly
marginalizing over foreground contaminants. Unlike
Fourier-transform-spectrometer-based designs, the specific bands and their
individual sensitivities can be independently set in this instrument, allowing
significant flexibility. We forecast SPECTER to observe the
$\Lambda$CDM-predicted $\mu$-distortion at $\approx 5\sigma$ (10$\sigma$)
assuming an observation time of 1 (4) year(s) (corresponding to mission
duration of 2 (8) years), after foreground marginalization. Our optimized
configuration includes 16 bands spanning 1-2000 GHz with degree-scale angular
resolution at 150 GHz and 1046 total detectors. SPECTER will additionally
measure the y-distortion at sub-percent precision and its relativistic
correction at percent-level precision, yielding tight constraints on the total
thermal energy and mean temperature of ionized gas.",http://arxiv.org/pdf/2409.12188v1,http://arxiv.org/abs/2409.12188v1
Exoplanet accretion monitoring spectroscopic survey (ENTROPY) I. Evidence for magnetospheric accretion in the young isolated planetary-mass object 2MASS J11151597+1937266,"Accretion among planets is a poorly understood phenomenon, due to lack of
both observational and theoretical studies. Detection of emission lines from
accreting gas giants facilitate detailed investigations into this process. This
work presents a detailed analysis of Balmer lines from one of the few known
young, planetary-mass objects with observed emission, the isolated L2 dwarf
2MASS J11151597+1937266 with a mass 7-21 Mj and age 5-45 Myr, located at 45+-2
pc. We obtained the first high-resolution (R~50,000) spectrum of the target
with VLT/UVES, a spectrograph in the near-UV to visible wavelengths (3200-6800
AA). We report resolved H3-H6 and He I (5875.6 AA) emission in the spectrum.
Based on the asymmetric line profiles of H3 and H4, 10% width of H3 (199+-1
km/s), tentative He I 6678 AA emission and indications of a disk from MIR
excess, we confirm ongoing accretion at this object. Using the Gaia update of
the parallax, we revise its temperature to 1816+-63 K and radius to 1.5+-0.1
Rj. Analysis of observed H I profiles using 1D planet-surface shock model
implies a pre-shock gas velocity of v0=120(+80,-40) km/s and a pre-shock
density of log(n0/cm^-3)=14(+0,-5). Pre-shock velocity points to a mass of
6(+8,-4) Mj for the target. Combining the H I line luminosities and planetary
Lline-Lacc scaling relations, we derive a mass accretion rate of
1.4(+2.8,-0.9)x10^-8 Mj/yr.",http://arxiv.org/pdf/2409.12187v1,http://arxiv.org/abs/2409.12187v1
Qwen2.5-Coder Technical Report,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade
from its predecessor, CodeQwen1.5. This series includes two models:
Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model,
Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained
on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning,
scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder
demonstrates impressive code generation capabilities while retaining general
versatility. The model has been evaluated on a wide range of code-related
tasks, achieving state-of-the-art (SOTA) performance across more than 10
benchmarks, including code generation, completion, reasoning, and repair,
consistently outperforming larger models of the same model size. We believe
that the release of the Qwen2.5-Coder series will not only push the boundaries
of research in code intelligence but also, through its permissive licensing,
encourage broader adoption by developers in real-world applications.",http://arxiv.org/pdf/2409.12186v1,http://arxiv.org/abs/2409.12186v1
To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning,"Chain-of-thought (CoT) via prompting is the de facto method for eliciting
reasoning capabilities from large language models (LLMs). But for what kinds of
tasks is this extra ``thinking'' really helpful? To analyze this, we conducted
a quantitative meta-analysis covering over 100 papers using CoT and ran our own
evaluations of 20 datasets across 14 models. Our results show that CoT gives
strong performance benefits primarily on tasks involving math or logic, with
much smaller gains on other types of tasks. On MMLU, directly generating the
answer without CoT leads to almost identical accuracy as CoT unless the
question or model's response contains an equals sign, indicating symbolic
operations and reasoning. Following this finding, we analyze the behavior of
CoT on these problems by separating planning and execution and comparing
against tool-augmented LLMs. Much of CoT's gain comes from improving symbolic
execution, but it underperforms relative to using a symbolic solver. Our
results indicate that CoT can be applied selectively, maintaining performance
while saving inference costs. Furthermore, they suggest a need to move beyond
prompt-based CoT to new paradigms that better leverage intermediate computation
across the whole range of LLM applications.",http://arxiv.org/pdf/2409.12183v1,http://arxiv.org/abs/2409.12183v1
A Controlled Study on Long Context Extension and Generalization in LLMs,"Broad textual understanding and in-context learning require language models
that utilize full document contexts. Due to the implementation challenges
associated with directly training long-context models, many methods have been
proposed for extending models to handle long contexts. However, owing to
differences in data and model classes, it has been challenging to compare these
approaches, leading to uncertainty as to how to evaluate long-context
performance and whether it differs from standard evaluation. We implement a
controlled protocol for extension methods with a standardized evaluation,
utilizing consistent base models and extension data. Our study yields several
insights into long-context behavior. First, we reaffirm the critical role of
perplexity as a general-purpose performance indicator even in longer-context
tasks. Second, we find that current approximate attention methods
systematically underperform across long-context tasks. Finally, we confirm that
exact fine-tuning based methods are generally effective within the range of
their extension, whereas extrapolation remains challenging. All codebases,
models, and checkpoints will be made available open-source, promoting
transparency and facilitating further research in this critical area of AI
development.",http://arxiv.org/pdf/2409.12181v1,http://arxiv.org/abs/2409.12181v1
Finetuning Language Models to Emit Linguistic Expressions of Uncertainty,"Large language models (LLMs) are increasingly employed in information-seeking
and decision-making tasks. Despite their broad utility, LLMs tend to generate
information that conflicts with real-world facts, and their persuasive style
can make these inaccuracies appear confident and convincing. As a result,
end-users struggle to consistently align the confidence expressed by LLMs with
the accuracy of their predictions, often leading to either blind trust in all
outputs or a complete disregard for their reliability. In this work, we explore
supervised finetuning on uncertainty-augmented predictions as a method to
develop models that produce linguistic expressions of uncertainty.
Specifically, we measure the calibration of pre-trained models and then
fine-tune language models to generate calibrated linguistic expressions of
uncertainty. Through experiments on various question-answering datasets, we
demonstrate that LLMs are well-calibrated in assessing their predictions, and
supervised finetuning based on the model's own confidence leads to
well-calibrated expressions of uncertainty, particularly for single-claim
answers.",http://arxiv.org/pdf/2409.12180v1,http://arxiv.org/abs/2409.12180v1
Poisson approximate likelihood compared to the particle filter,"Filtering algorithms are fundamental for inference on partially observed
stochastic dynamic systems, since they provide access to the likelihood
function and hence enable likelihood-based or Bayesian inference. A novel
Poisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.
(2023). PAL employs a Poisson approximation to conditional densities, offering
a fast approximation to the likelihood function for a certain subset of
partially observed Markov process models. A central piece of evidence for PAL
is the comparison in Table 1 of Whitehouse et al. (2023), which claims a large
improvement for PAL over a standard particle filter algorithm. This evidence,
based on a model and data from a previous scientific study by Stocks et al.
(2020), might suggest that researchers confronted with similar models should
use PAL rather than particle filter methods. Taken at face value, this evidence
also reduces the credibility of Stocks et al. (2020) by indicating a
shortcoming with the numerical methods that they used. However, we show that
the comparison of log-likelihood values made by Whitehouse et al. (2023) is
flawed because their PAL calculations were carried out using a dataset scaled
differently from the previous study. If PAL and the particle filter are applied
to the same data, the advantage claimed for PAL disappears. On simulations
where the model is correctly specified, the particle filter outperforms PAL.",http://arxiv.org/pdf/2409.12173v1,http://arxiv.org/abs/2409.12173v1
You Only Read Once (YORO): Learning to Internalize Database Knowledge for Text-to-SQL,"While significant progress has been made on the text-to-SQL task, recent
solutions repeatedly encode the same database schema for every question,
resulting in unnecessary high inference cost and often overlooking crucial
database knowledge. To address these issues, we propose You Only Read Once
(YORO), a novel paradigm that directly internalizes database knowledge into the
parametric knowledge of a text-to-SQL model during training and eliminates the
need for schema encoding during inference. YORO significantly reduces the input
token length by 66%-98%. Despite its shorter inputs, our empirical results
demonstrate YORO's competitive performances with traditional systems on three
benchmarks as well as its significant outperformance on large databases.
Furthermore, YORO excels in handling questions with challenging value
retrievals such as abbreviation.",http://arxiv.org/pdf/2409.12172v1,http://arxiv.org/abs/2409.12172v1
multiPI-TransBTS: A Multi-Path Learning Framework for Brain Tumor Image Segmentation Based on Multi-Physical Information,"Brain Tumor Segmentation (BraTS) plays a critical role in clinical diagnosis,
treatment planning, and monitoring the progression of brain tumors. However,
due to the variability in tumor appearance, size, and intensity across
different MRI modalities, automated segmentation remains a challenging task. In
this study, we propose a novel Transformer-based framework, multiPI-TransBTS,
which integrates multi-physical information to enhance segmentation accuracy.
The model leverages spatial information, semantic information, and multi-modal
imaging data, addressing the inherent heterogeneity in brain tumor
characteristics. The multiPI-TransBTS framework consists of an encoder, an
Adaptive Feature Fusion (AFF) module, and a multi-source, multi-scale feature
decoder. The encoder incorporates a multi-branch architecture to separately
extract modality-specific features from different MRI sequences. The AFF module
fuses information from multiple sources using channel-wise and element-wise
attention, ensuring effective feature recalibration. The decoder combines both
common and task-specific features through a Task-Specific Feature Introduction
(TSFI) strategy, producing accurate segmentation outputs for Whole Tumor (WT),
Tumor Core (TC), and Enhancing Tumor (ET) regions. Comprehensive evaluations on
the BraTS2019 and BraTS2020 datasets demonstrate the superiority of
multiPI-TransBTS over the state-of-the-art methods. The model consistently
achieves better Dice coefficients, Hausdorff distances, and Sensitivity scores,
highlighting its effectiveness in addressing the BraTS challenges. Our results
also indicate the need for further exploration of the balance between precision
and recall in the ET segmentation task. The proposed framework represents a
significant advancement in BraTS, with potential implications for improving
clinical outcomes for brain tumor patients.",http://arxiv.org/pdf/2409.12167v1,http://arxiv.org/abs/2409.12167v1
Synchrotron self-Compton in a radiative-adiabatic fireball scenario: Modelling the multiwavelength observations in some Fermi/LAT bursts,"Energetic GeV photons expected from the closest and the most energetic
Gamma-ray bursts (GRBs) provide an unique opportunity to study the
very-high-energy emission as well as the possible correlations with lower
energy bands in realistic GRB afterglow models. In the standard GRB afterglow
model, the relativistic homogeneous shock is usually considered to be fully
adiabatic, however, it could be partially radiative. Based on the external
forward-shock scenario in both stellar wind and constant-density medium. We
present a radiative-adiabatic analytical model of the synchrotron self-Compton
(SSC) and synchrotron processes considering an electron energy distribution
with a power-law index of 1 < p < 2 and 2 $\leq$ p. We show that the SSC
scenario plays a relevant role in the radiative parameter $\epsilon$, leading
to a prolonged evolution during the slow cooling regime. In a particular case,
we derive the Fermi/LAT light curves together with the photons with energies
$\geq$ 100 MeV in a sample of nine bursts from the second Fermi/LAT GRB catalog
that exhibited temporal and spectral indices with $\geq$ 1.5 and $\approx$ 2,
respectively. These events can hardly be described with closure relations of
the standard synchrotron afterglow model, and also exhibit energetic photons
above the synchrotron limit. We have modeled the multi-wavelength observations
of our sample to constrain the microphysical parameters, the circumburst
density, the bulk Lorentz factor and the mechanism responsible for explaining
the energetic GeV photons.",http://arxiv.org/pdf/2409.12166v1,http://arxiv.org/abs/2409.12166v1
Blind Deconvolution on Graphs: Exact and Stable Recovery,"We study a blind deconvolution problem on graphs, which arises in the context
of localizing a few sources that diffuse over networks. While the observations
are bilinear functions of the unknown graph filter coefficients and sparse
input signals, a mild requirement on invertibility of the diffusion filter
enables an efficient convex relaxation leading to a linear programming
formulation that can be tackled with off-the-shelf solvers. Under the
Bernoulli-Gaussian model for the inputs, we derive sufficient exact recovery
conditions in the noise-free setting. A stable recovery result is then
established, ensuring the estimation error remains manageable even when the
observations are corrupted by a small amount of noise. Numerical tests with
synthetic and real-world network data illustrate the merits of the proposed
algorithm, its robustness to noise as well as the benefits of leveraging
multiple signals to aid the (blind) localization of sources of diffusion. At a
fundamental level, the results presented here broaden the scope of classical
blind deconvolution of (spatio-)temporal signals to irregular graph domains.",http://arxiv.org/pdf/2409.12164v1,http://arxiv.org/abs/2409.12164v1
Identifying inflated Fermi surfaces via thermoelectric response in $d$-wave superconductor heterostructure,"We theoretically investigate the thermoelectric response of inflated Fermi
surfaces (IFSs) generated in a two dimensional unconventional $d$-wave
superconductor subjected to an external in-plane Zeeman field. These IFSs
exhibiting the same dimension as the underlying normal state Fermi surface are
topologically protected by combinations of discrete symmetries. Utilizing the
Blonder-Tinkham-Klapwijk formalism and considering normal-$d$-wave
superconductor hybrid junction, we compute the thermoelectric coefficients
including thermal conductance, Seebeck coefficient, figure of merit ($zT$), and
examine the validation of Widemann-Franz law in the presence of both voltage
and temperature bias. Importantly, as a signature of anisotropic nature of
$d$-wave pairing, Andreev bound states (ABSs) formed at the
normal-superconductor interface play a significant role in the thermoelectric
response. In the presence of ABSs, we observe a substantial enhancement in
Seebeck coefficient ($\sim 200\,\mu$V/K) and $zT$ ($\sim 3.5$) due to the
generation of the IFSs and thus making such setup a potential candidate for
device applications. Finally, we strengthen our continuum model results by
computing the thermoelectric coefficients based on a lattice-regularized
version of our continuum model.",http://arxiv.org/pdf/2409.12157v1,http://arxiv.org/abs/2409.12157v1
Autopet III challenge: Incorporating anatomical knowledge into nnUNet for lesion segmentation in PET/CT,"Lesion segmentation in PET/CT imaging is essential for precise tumor
characterization, which supports personalized treatment planning and enhances
diagnostic precision in oncology. However, accurate manual segmentation of
lesions is time-consuming and prone to inter-observer variability. Given the
rising demand and clinical use of PET/CT, automated segmentation methods,
particularly deep-learning-based approaches, have become increasingly more
relevant. The autoPET III Challenge focuses on advancing automated segmentation
of tumor lesions in PET/CT images in a multitracer multicenter setting,
addressing the clinical need for quantitative, robust, and generalizable
solutions. Building on previous challenges, the third iteration of the autoPET
challenge introduces a more diverse dataset featuring two different tracers
(FDG and PSMA) from two clinical centers. To this extent, we developed a
classifier that identifies the tracer of the given PET/CT based on the Maximum
Intensity Projection of the PET scan. We trained two individual
nnUNet-ensembles for each tracer where anatomical labels are included as a
multi-label task to enhance the model's performance. Our final submission
achieves cross-validation Dice scores of 76.90% and 61.33% for the publicly
available FDG and PSMA datasets, respectively. The code is available at
https://github.com/hakal104/autoPETIII/ .",http://arxiv.org/pdf/2409.12155v1,http://arxiv.org/abs/2409.12155v1
Robots that Learn to Safely Influence via Prediction-Informed Reach-Avoid Dynamic Games,"Robots can influence people to accomplish their tasks more efficiently:
autonomous cars can inch forward at an intersection to pass through, and
tabletop manipulators can go for an object on the table first. However, a
robot's ability to influence can also compromise the safety of nearby people if
naively executed. In this work, we pose and solve a novel robust reach-avoid
dynamic game which enables robots to be maximally influential, but only when a
safety backup control exists. On the human side, we model the human's behavior
as goal-driven but conditioned on the robot's plan, enabling us to capture
influence. On the robot side, we solve the dynamic game in the joint physical
and belief space, enabling the robot to reason about how its uncertainty in
human behavior will evolve over time. We instantiate our method, called SLIDE
(Safely Leveraging Influence in Dynamic Environments), in a high-dimensional
(39-D) simulated human-robot collaborative manipulation task solved via offline
game-theoretic reinforcement learning. We compare our approach to a robust
baseline that treats the human as a worst-case adversary, a safety controller
that does not explicitly reason about influence, and an energy-function-based
safety shield. We find that SLIDE consistently enables the robot to leverage
the influence it has on the human when it is safe to do so, ultimately allowing
the robot to be less conservative while still ensuring a high safety rate
during task execution.",http://arxiv.org/pdf/2409.12153v1,http://arxiv.org/abs/2409.12153v1
Decoding Style: Efficient Fine-Tuning of LLMs for Image-Guided Outfit Recommendation with Preference,"Personalized outfit recommendation remains a complex challenge, demanding
both fashion compatibility understanding and trend awareness. This paper
presents a novel framework that harnesses the expressive power of large
language models (LLMs) for this task, mitigating their ""black box"" and static
nature through fine-tuning and direct feedback integration. We bridge the item
visual-textual gap in items descriptions by employing image captioning with a
Multimodal Large Language Model (MLLM). This enables the LLM to extract style
and color characteristics from human-curated fashion images, forming the basis
for personalized recommendations. The LLM is efficiently fine-tuned on the
open-source Polyvore dataset of curated fashion images, optimizing its ability
to recommend stylish outfits. A direct preference mechanism using negative
examples is employed to enhance the LLM's decision-making process. This creates
a self-enhancing AI feedback loop that continuously refines recommendations in
line with seasonal fashion trends. Our framework is evaluated on the Polyvore
dataset, demonstrating its effectiveness in two key tasks: fill-in-the-blank,
and complementary item retrieval. These evaluations underline the framework's
ability to generate stylish, trend-aligned outfit suggestions, continuously
improving through direct feedback. The evaluation results demonstrated that our
proposed framework significantly outperforms the base LLM, creating more
cohesive outfits. The improved performance in these tasks underscores the
proposed framework's potential to enhance the shopping experience with accurate
suggestions, proving its effectiveness over the vanilla LLM based outfit
generation.",http://arxiv.org/pdf/2409.12150v1,http://arxiv.org/abs/2409.12150v1
"MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning","Large Language Models' (LLM) reasoning can be improved using test-time
aggregation strategies, i.e., generating multiple samples and voting among
generated samples. While these improve performance, they often reach a
saturation point. Refinement offers an alternative by using LLM-generated
feedback to improve solution quality. However, refinement introduces 3 key
challenges: (1) Excessive refinement: Uniformly refining all instances can
over-correct and reduce the overall performance. (2) Inability to localize and
address errors: LLMs have a limited ability to self-correct and struggle to
identify and correct their own mistakes. (3) Insufficient refinement: Deciding
how many iterations of refinement are needed is non-trivial, and stopping too
soon could leave errors unaddressed. To tackle these issues, we propose
MAgICoRe, which avoids excessive refinement by categorizing problem difficulty
as easy or hard, solving easy problems with coarse-grained aggregation and hard
ones with fine-grained and iterative multi-agent refinement. To improve error
localization, we incorporate external step-wise reward model (RM) scores.
Moreover, to ensure effective refinement, we employ a multi-agent loop with
three agents: Solver, Reviewer (which generates targeted feedback based on
step-wise RM scores), and the Refiner (which incorporates feedback). To ensure
sufficient refinement, we re-evaluate updated solutions, iteratively initiating
further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5
and show its effectiveness across 5 math datasets. Even one iteration of
MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by
4.0% while using less than half the samples. Unlike iterative refinement with
baselines, MAgICoRe continues to improve with more iterations. Finally, our
ablations highlight the importance of MAgICoRe's RMs and multi-agent
communication.",http://arxiv.org/pdf/2409.12147v1,http://arxiv.org/abs/2409.12147v1
Lempel-Ziv (LZ77) Factorization in Sublinear Time,"Lempel-Ziv (LZ77) factorization is a fundamental problem in string
processing: Greedily partition a given string $T$ from left to right into
blocks (called phrases) so that each phrase is either the leftmost occurrence
of a letter or the longest prefix of the unprocessed suffix that has another
occurrence earlier in $T$. Due to numerous applications, LZ77 factorization is
one of the most studied problems on strings. In the 47 years since its
inception, several algorithms were developed for different models of
computation, including parallel, GPU, external-memory, and quantum. Remarkably,
however, the complexity of the most basic variant is still not settled: All
existing algorithms in the RAM model run in $\Omega(n)$ time, which is a
$\Theta(\log n)$ factor away from the lower bound of $\Omega(n/\log n)$
(following from the necessity to read the input, which takes $\Theta(n/\log n)$
space for $T\in\{0,1\}^{n}$).
  We present the first $o(n)$-time algorithm for LZ77 factorization, breaking
the linear-time barrier present for nearly 50 years. For $T\in\{0,1\}^{n}$, our
algorithm runs in $\mathcal{O}(n/\sqrt{\log n})=o(n)$ time and uses the optimal
$\mathcal{O}(n/\log n)$ working space. Our algorithm generalizes to
$\Sigma=[0..\sigma)$, where $\sigma=n^{\mathcal{O}(1)}$. The runtime and
working space then become $\mathcal{O}((n\log\sigma)/\sqrt{\log n})$ and
$\mathcal{O}(n/\log_{\sigma} n)$. To obtain our algorithm, we prove a more
general result: For any constant $\epsilon\in(0,1)$ and $T\in[0..\sigma)^{n}$,
in $\mathcal{O}((n\log\sigma)/\sqrt{\log n})$ time and using
$\mathcal{O}(n/\log_{\sigma}n)$ space, we can construct an
$\mathcal{O}(n/\log_{\sigma}n)$-size index that, given any $P=T[j..j+\ell)$
(represented as $(j,\ell)$), computes the leftmost occurrence of $P$ in $T$ in
$\mathcal{O}(\log^{\epsilon}n)$ time. In other words, we solve the
indexing/online variant of the LZ77 problem.",http://arxiv.org/pdf/2409.12146v1,http://arxiv.org/abs/2409.12146v1
Experimental Evidence That Conversational Artificial Intelligence Can Steer Consumer Behavior Without Detection,"Conversational AI models are becoming increasingly popular and are about to
replace traditional search engines for information retrieval and product
discovery. This raises concerns about monetization strategies and the potential
for subtle consumer manipulation. Companies may have financial incentives to
steer users toward search results or products in a conversation in ways that
are unnoticeable to consumers. Using a behavioral experiment, we show that
conversational AI models can indeed significantly shift consumer preferences.
We discuss implications and ask whether regulators are sufficiently prepared to
combat potential consumer deception.",http://arxiv.org/pdf/2409.12143v1,http://arxiv.org/abs/2409.12143v1
Investigating the effects of precise mass measurements of Ru and Pd isotopes on machine learning mass modeling,"Atomic masses are a foundational quantity in our understanding of nuclear
structure, astrophysics and fundamental symmetries. The long-standing goal of
creating a predictive global model for the binding energy of a nucleus remains
a significant challenge, however, and prompts the need for precise measurements
of atomic masses to serve as anchor points for model developments. We present
precise mass measurements of neutron-rich Ru and Pd isotopes performed at the
Californium Rare Isotope Breeder Upgrade facility at Argonne National
Laboratory using the Canadian Penning Trap mass spectrometer. The masses of
$^{108}$Ru, $^{110}$Ru and $^{116}$Pd were measured to a relative mass
precision $\delta m/m \approx 10^{-8}$ via the phase-imaging
ion-cyclotron-resonance technique, and represent an improvement of
approximately an order of magnitude over previous measurements. These mass data
were used in conjunction with the physically interpretable machine learning
(PIML) model, which uses a mixture density neural network to model mass
excesses via a mixture of Gaussian distributions. The effects of our new mass
data on a Bayesian-updating of a PIML model are presented.",http://arxiv.org/pdf/2409.12141v1,http://arxiv.org/abs/2409.12141v1
MoRAG -- Multi-Fusion Retrieval Augmented Generation for Human Motion,"We introduce MoRAG, a novel multi-part fusion based retrieval-augmented
generation strategy for text-based human motion generation. The method enhances
motion diffusion models by leveraging additional knowledge obtained through an
improved motion retrieval process. By effectively prompting large language
models (LLMs), we address spelling errors and rephrasing issues in motion
retrieval. Our approach utilizes a multi-part retrieval strategy to improve the
generalizability of motion retrieval across the language space. We create
diverse samples through the spatial composition of the retrieved motions.
Furthermore, by utilizing low-level, part-specific motion information, we can
construct motion samples for unseen text descriptions. Our experiments
demonstrate that our framework can serve as a plug-and-play module, improving
the performance of motion diffusion models. Code, pretrained models and sample
videos will be made available at: https://motion-rag.github.io/",http://arxiv.org/pdf/2409.12140v1,http://arxiv.org/abs/2409.12140v1
Takin: A Cohort of Superior Quality Zero-shot Speech Generation Models,"With the advent of the big data and large language model era, zero-shot
personalized rapid customization has emerged as a significant trend. In this
report, we introduce Takin AudioLLM, a series of techniques and models, mainly
including Takin TTS, Takin VC, and Takin Morphing, specifically designed for
audiobook production. These models are capable of zero-shot speech production,
generating high-quality speech that is nearly indistinguishable from real human
speech and facilitating individuals to customize the speech content according
to their own needs. Specifically, we first introduce Takin TTS, a neural codec
language model that builds upon an enhanced neural speech codec and a
multi-task training framework, capable of generating high-fidelity natural
speech in a zero-shot way. For Takin VC, we advocate an effective content and
timbre joint modeling approach to improve the speaker similarity, while
advocating for a conditional flow matching based decoder to further enhance its
naturalness and expressiveness. Last, we propose the Takin Morphing system with
highly decoupled and advanced timbre and prosody modeling approaches, which
enables individuals to customize speech production with their preferred timbre
and prosody in a precise and controllable manner. Extensive experiments
validate the effectiveness and robustness of our Takin AudioLLM series models.
For detailed demos, please refer to https://takinaudiollm.github.io.",http://arxiv.org/pdf/2409.12139v1,http://arxiv.org/abs/2409.12139v1
GRIN: GRadient-INformed MoE,"Mixture-of-Experts (MoE) models scale more effectively than dense models due
to sparse computation through expert routing, selectively activating only a
small subset of expert modules. However, sparse computation challenges
traditional training practices, as discrete expert routing hinders standard
backpropagation and thus gradient-based optimization, which are the cornerstone
of deep learning. To better pursue the scaling power of MoE, we introduce GRIN
(GRadient-INformed MoE training), which incorporates sparse gradient estimation
for expert routing and configures model parallelism to avoid token dropping.
Applying GRIN to autoregressive language modeling, we develop a top-2
16$\times$3.8B MoE model. Our model, with only 6.6B activated parameters,
outperforms a 7B dense model and matches the performance of a 14B dense model
trained on the same data. Extensive evaluations across diverse tasks
demonstrate the potential of GRIN to significantly enhance MoE efficacy,
achieving 79.4 on MMLU, 83.7 on HellaSwag, 74.4 on HumanEval, and 58.9 on MATH.",http://arxiv.org/pdf/2409.12136v1,http://arxiv.org/abs/2409.12136v1
BERT-VBD: Vietnamese Multi-Document Summarization Framework,"In tackling the challenge of Multi-Document Summarization (MDS), numerous
methods have been proposed, spanning both extractive and abstractive
summarization techniques. However, each approach has its own limitations,
making it less effective to rely solely on either one. An emerging and
promising strategy involves a synergistic fusion of extractive and abstractive
summarization methods. Despite the plethora of studies in this domain, research
on the combined methodology remains scarce, particularly in the context of
Vietnamese language processing. This paper presents a novel Vietnamese MDS
framework leveraging a two-component pipeline architecture that integrates
extractive and abstractive techniques. The first component employs an
extractive approach to identify key sentences within each document. This is
achieved by a modification of the pre-trained BERT network, which derives
semantically meaningful phrase embeddings using siamese and triplet network
structures. The second component utilizes the VBD-LLaMA2-7B-50b model for
abstractive summarization, ultimately generating the final summary document.
Our proposed framework demonstrates a positive performance, attaining ROUGE-2
scores of 39.6% on the VN-MDS dataset and outperforming the state-of-the-art
baselines.",http://arxiv.org/pdf/2409.12134v1,http://arxiv.org/abs/2409.12134v1
Einstein-dilaton-four-Maxwell Holographic Anisotropic Models,"In recent literature on holographic QCD, the consideration of the
five-dimensional Einstein-dilaton-Maxwell models has played a crucial role.
Typically, one Maxwell field is associated with the chemical potential, while
additional Maxwell fields are used to describe the anisotropy of the model. A
more general scenario involves up to four Maxwell fields. The second field
represents spatial longitudinal-transverse anisotropy, while the third and
fourth fields describe anisotropy induced by an external magnetic field. We
consider an ansatz for the metric characterized by four functions at zero
temperature and five functions at non-zero temperature. Maxwell field related
to the chemical potential is treated with the electric ansatz, as is customary,
whereas the remaining three Maxwell fields are treated with a magnetic ansatz.
We demonstrate that for the fully anisotropic diagonal metric only six out of
the seven equations are independent. One of the matter equations -- either the
dilaton or the vector potential equation -- follows from the Einstein equations
and the remaining matter equation. This redundancy arises due to the Bianchi
identity for the Einstein tensor and the specific form of the stress-energy
tensor in the model. A procedure for solving this system of six equations is
provided. This method generalizes previously studied cases involving up to
three Maxwell fields. In the solution with three magnetic fields our analysis
shows, that the dilaton equation is a consequence of the five Einstein
equations and the equation for the vector potential",http://arxiv.org/pdf/2409.12131v1,http://arxiv.org/abs/2409.12131v1
Linguini: A benchmark for language-agnostic linguistic reasoning,"We propose a new benchmark to measure a language model's linguistic reasoning
skills without relying on pre-existing language-specific knowledge. The test
covers 894 questions grouped in 160 problems across 75 (mostly) extremely
low-resource languages, extracted from the International Linguistic Olympiad
corpus. To attain high accuracy on this benchmark, models don't need previous
knowledge of the tested language, as all the information needed to solve the
linguistic puzzle is presented in the context. We find that, while all analyzed
models rank below 25% accuracy, there is a significant gap between open and
closed models, with the best-performing proprietary model at 24.05% and the
best-performing open model at 8.84%.",http://arxiv.org/pdf/2409.12126v1,http://arxiv.org/abs/2409.12126v1
Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement,"In this report, we present a series of math-specific large language models:
Qwen2.5-Math and Qwen2.5-Math-Instruct-1.5B/7B/72B. The core innovation of the
Qwen2.5 series lies in integrating the philosophy of self-improvement
throughout the entire pipeline, from pre-training and post-training to
inference: (1) During the pre-training phase, Qwen2-Math-Instruct is utilized
to generate large-scale, high-quality mathematical data. (2) In the
post-training phase, we develop a reward model (RM) by conducting massive
sampling from Qwen2-Math-Instruct. This RM is then applied to the iterative
evolution of data in supervised fine-tuning (SFT). With a stronger SFT model,
it's possible to iteratively train and update the RM, which in turn guides the
next round of SFT data iteration. On the final SFT model, we employ the
ultimate RM for reinforcement learning, resulting in the Qwen2.5-Math-Instruct.
(3) Furthermore, during the inference stage, the RM is used to guide sampling,
optimizing the model's performance.
  Qwen2.5-Math-Instruct supports both Chinese and English, and possess advanced
mathematical reasoning capabilities, including Chain-of-Thought (CoT) and
Tool-Integrated Reasoning (TIR). We evaluate our models on 10 mathematics
datasets in both English and Chinese, such as GSM8K, MATH, GaoKao, AMC23, and
AIME24, covering a range of difficulties from grade school level to math
competition problems.",http://arxiv.org/pdf/2409.12122v1,http://arxiv.org/abs/2409.12122v1
Computing the $\mathbb{Z}_2$ Invariant in Two-Dimensional Strongly-Correlated Systems,"We show that the two-dimensional $\mathbb{Z}_2$ invariant for time-reversal
invariant insulators can be formulated in terms of the boundary-condition
dependence of the ground state wavefunction for both non-interacting and
strongly-correlated insulators. By introducing a family of quasi-single
particle states associated to the many-body ground state of an insulator, we
show that the $\mathbb{Z}_2$ invariant can be expressed as the integral of a
certain Berry connection over half the space of boundary conditions, providing
an alternative expression to the formulations that appear in [Lee et al., Phys.
Rev. Lett. $\textbf{100}$, 186807 (2008)]. We show the equivalence of the
different many-body formulations of the invariant, and show how they reduce to
known band-theoretic results for Slater determinant ground states. Finally, we
apply our results to analytically calculate the invariant for the Kane-Mele
model with nonlocal (orbital) Hatsugai-Kohmoto (HK) interactions. This
rigorously establishes the topological nontriviality of the Kane-Mele model
with HK interactions, and represents one of the few exact calculations of the
$\mathbb{Z}_2$ invariant for a strongly-interacting system.",http://arxiv.org/pdf/2409.12120v1,http://arxiv.org/abs/2409.12120v1
Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference,"Large language models (LLMs) have significantly advanced audio processing
through audio codecs that convert audio into discrete tokens, enabling the
application of language modeling techniques to audio data. However, audio
codecs often operate at high frame rates, resulting in slow training and
inference, especially for autoregressive models. To address this challenge, we
present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that
leverages finite scalar quantization and adversarial training with large speech
language models to achieve high-quality audio compression with a 1.89 kbps
bitrate and 21.5 frames per second. We demonstrate that our novel codec can
make the inference of LLM-based text-to-speech models around three times faster
while improving intelligibility and producing quality comparable to previous
models.",http://arxiv.org/pdf/2409.12117v1,http://arxiv.org/abs/2409.12117v1
Stronger Baseline Models -- A Key Requirement for Aligning Machine Learning Research with Clinical Utility,"Machine Learning (ML) research has increased substantially in recent years,
due to the success of predictive modeling across diverse application domains.
However, well-known barriers exist when attempting to deploy ML models in
high-stakes, clinical settings, including lack of model transparency (or the
inability to audit the inference process), large training data requirements
with siloed data sources, and complicated metrics for measuring model utility.
In this work, we show empirically that including stronger baseline models in
healthcare ML evaluations has important downstream effects that aid
practitioners in addressing these challenges. Through a series of case studies,
we find that the common practice of omitting baselines or comparing against a
weak baseline model (e.g. a linear model with no optimization) obscures the
value of ML methods proposed in the research literature. Using these insights,
we propose some best practices that will enable practitioners to more
effectively study and deploy ML models in clinical settings.",http://arxiv.org/pdf/2409.12116v1,http://arxiv.org/abs/2409.12116v1
Revisiting Dark Photon Constraints from CMB Spectral Distortions,"Spectral distortions of the cosmic microwave background (CMB) provide
stringent constraints on energy and entropy production in the post-BBN (Big
Bang Nucleosynthesis) era. This has been used to constrain dark photon models
with COBE/FIRAS and forecast the potential gains with future CMB spectrometers.
Here, we revisit these constraints by carefully considering the photon to dark
photon conversion process and evolution of the distortion signal. Previous
works only included the effect of CMB energy density changes but neglected the
change to the photon number density. We clearly define the dark photon
distortion signal and show that in contrast to previous analytic estimates the
distortion has an opposite sign and a $\simeq 1.5$ times larger amplitude. We
furthermore extend the treatment into the large distortion regime to also cover
the redshift range $\simeq 2\times 10^6-4\times 10^7$ between the $\mu$-era and
the end of BBN using CosmoTherm. This shows that the CMB distortion constraints
for dark photon masses in the range $10^{-4}\,{\rm eV}\lesssim m_{\rm
dp}\lesssim 10^{-3}\,{\rm eV}$ were significantly underestimated. We
demonstrate that in the small distortion regime the distortion caused by photon
to dark photon conversion is extremely close to a $\mu$-type distortion
independent of the conversion redshift. This opens the possibility to study
dark photon models using CMB distortion anisotropies and the correlations with
CMB temperature anisotropies as we highlight here.",http://arxiv.org/pdf/2409.12115v1,http://arxiv.org/abs/2409.12115v1
Applications of Knowledge Distillation in Remote Sensing: A Survey,"With the ever-growing complexity of models in the field of remote sensing
(RS), there is an increasing demand for solutions that balance model accuracy
with computational efficiency. Knowledge distillation (KD) has emerged as a
powerful tool to meet this need, enabling the transfer of knowledge from large,
complex models to smaller, more efficient ones without significant loss in
performance. This review article provides an extensive examination of KD and
its innovative applications in RS. KD, a technique developed to transfer
knowledge from a complex, often cumbersome model (teacher) to a more compact
and efficient model (student), has seen significant evolution and application
across various domains. Initially, we introduce the fundamental concepts and
historical progression of KD methods. The advantages of employing KD are
highlighted, particularly in terms of model compression, enhanced computational
efficiency, and improved performance, which are pivotal for practical
deployments in RS scenarios. The article provides a comprehensive taxonomy of
KD techniques, where each category is critically analyzed to demonstrate the
breadth and depth of the alternative options, and illustrates specific case
studies that showcase the practical implementation of KD methods in RS tasks,
such as instance segmentation and object detection. Further, the review
discusses the challenges and limitations of KD in RS, including practical
constraints and prospective future directions, providing a comprehensive
overview for researchers and practitioners in the field of RS. Through this
organization, the paper not only elucidates the current state of research in KD
but also sets the stage for future research opportunities, thereby contributing
significantly to both academic research and real-world applications.",http://arxiv.org/pdf/2409.12111v1,http://arxiv.org/abs/2409.12111v1
SPRMamba: Surgical Phase Recognition for Endoscopic Submucosal Dissection with Mamba,"Endoscopic Submucosal Dissection (ESD) is a minimally invasive procedure
initially designed for the treatment of early gastric cancer but is now widely
used for various gastrointestinal lesions. Computer-assisted Surgery systems
have played a crucial role in improving the precision and safety of ESD
procedures, however, their effectiveness is limited by the accurate recognition
of surgical phases. The intricate nature of ESD, with different lesion
characteristics and tissue structures, presents challenges for real-time
surgical phase recognition algorithms. Existing surgical phase recognition
algorithms struggle to efficiently capture temporal contexts in video-based
scenarios, leading to insufficient performance. To address these issues, we
propose SPRMamba, a novel Mamba-based framework for ESD surgical phase
recognition. SPRMamba leverages the strengths of Mamba for long-term temporal
modeling while introducing the Scaled Residual TranMamba block to enhance the
capture of fine-grained details, overcoming the limitations of traditional
temporal models like Temporal Convolutional Networks and Transformers.
Moreover, a Temporal Sample Strategy is introduced to accelerate the
processing, which is essential for real-time phase recognition in clinical
settings. Extensive testing on the ESD385 dataset and the cholecystectomy
Cholec80 dataset demonstrates that SPRMamba surpasses existing state-of-the-art
methods and exhibits greater robustness across various surgical phase
recognition tasks.",http://arxiv.org/pdf/2409.12108v1,http://arxiv.org/abs/2409.12108v1
Measuring Human and AI Values based on Generative Psychometrics with Large Language Models,"Human values and their measurement are long-standing interdisciplinary
inquiry. Recent advances in AI have sparked renewed interest in this area, with
large language models (LLMs) emerging as both tools and subjects of value
measurement. This work introduces Generative Psychometrics for Values (GPV), an
LLM-based, data-driven value measurement paradigm, theoretically grounded in
text-revealed selective perceptions. We begin by fine-tuning an LLM for
accurate perception-level value measurement and verifying the capability of
LLMs to parse texts into perceptions, forming the core of the GPV pipeline.
Applying GPV to human-authored blogs, we demonstrate its stability, validity,
and superiority over prior psychological tools. Then, extending GPV to LLM
value measurement, we advance the current art with 1) a psychometric
methodology that measures LLM values based on their scalable and free-form
outputs, enabling context-specific measurement; 2) a comparative analysis of
measurement paradigms, indicating response biases of prior methods; and 3) an
attempt to bridge LLM values and their safety, revealing the predictive power
of different value systems and the impacts of various values on LLM safety.
Through interdisciplinary efforts, we aim to leverage AI for next-generation
psychometrics and psychometrics for value-aligned AI.",http://arxiv.org/pdf/2409.12106v1,http://arxiv.org/abs/2409.12106v1
FedLF: Adaptive Logit Adjustment and Feature Optimization in Federated Long-Tailed Learning,"Federated learning offers a paradigm to the challenge of preserving privacy
in distributed machine learning. However, datasets distributed across each
client in the real world are inevitably heterogeneous, and if the datasets can
be globally aggregated, they tend to be long-tailed distributed, which greatly
affects the performance of the model. The traditional approach to federated
learning primarily addresses the heterogeneity of data among clients, yet it
fails to address the phenomenon of class-wise bias in global long-tailed data.
This results in the trained model focusing on the head classes while neglecting
the equally important tail classes. Consequently, it is essential to develop a
methodology that considers classes holistically. To address the above problems,
we propose a new method FedLF, which introduces three modifications in the
local training phase: adaptive logit adjustment, continuous class centred
optimization, and feature decorrelation. We compare seven state-of-the-art
methods with varying degrees of data heterogeneity and long-tailed
distribution. Extensive experiments on benchmark datasets CIFAR-10-LT and
CIFAR-100-LT demonstrate that our approach effectively mitigates the problem of
model performance degradation due to data heterogeneity and long-tailed
distribution. our code is available at https://github.com/18sym/FedLF.",http://arxiv.org/pdf/2409.12105v1,http://arxiv.org/abs/2409.12105v1
Performance of Quantum Approximate Optimization with Quantum Error Detection,"Quantum algorithms must be scaled up to tackle real-world applications. Doing
so requires overcoming the noise present on today's hardware. The quantum
approximate optimization algorithm (QAOA) is a promising candidate for scaling
up due to its modest resource requirements and documented asymptotic speedup
over state-of-the-art classical algorithms for some problems. However,
achieving better-than-classical performance with QAOA is believed to require
fault tolerance. In this paper, we demonstrate a partially fault-tolerant
implementation of QAOA using the $[[k+2,k,2]]$ ``Iceberg'' error detection
code. We observe that encoding the circuit with the Iceberg code improves the
algorithmic performance as compared to the unencoded circuit for problems with
up to $20$ logical qubits on a trapped-ion quantum computer. Additionally, we
propose and calibrate a model for predicting the code performance, and use it
to characterize the limits of the Iceberg code and extrapolate its performance
to future hardware with improved error rates. In particular, we show how our
model can be used to determine necessary conditions for QAOA to outperform
Goemans-Williamson algorithm on future hardware. Our results demonstrate the
largest universal quantum computing algorithm protected by partially
fault-tolerant quantum error detection on practical applications to date,
paving the way towards solving real-world applications with quantum computers.",http://arxiv.org/pdf/2409.12104v1,http://arxiv.org/abs/2409.12104v1
Cyclicity Analysis of the Ornstein-Uhlenbeck Process,"In this thesis, we consider an $N$-dimensional Ornstein-Uhlenbeck (OU)
process satisfying the linear stochastic differential equation $d\mathbf x(t) =
- \mathbf B\mathbf x(t) dt + \boldsymbol \Sigma d \mathbf w(t).$ Here, $\mathbf
B$ is a fixed $N \times N$ circulant friction matrix whose eigenvalues have
positive real parts, $\boldsymbol \Sigma$ is a fixed $N \times M$ matrix. We
consider a signal propagation model governed by this OU process. In this model,
an underlying signal propagates throughout a network consisting of $N$ linked
sensors located in space. We interpret the $n$-th component of the OU process
as the measurement of the propagating effect made by the $n$-th sensor. The
matrix $\mathbf B$ represents the sensor network structure: if $\mathbf B$ has
first row $(b_1 \ , \ \dots \ , \ b_N),$ where $b_1>0$ and $b_2 \ , \ \dots \
,\ b_N \le 0,$ then the magnitude of $b_p$ quantifies how receptive the $n$-th
sensor is to activity within the $(n+p-1)$-th sensor. Finally, the $(m,n)$-th
entry of the matrix $\mathbf D = \frac{\boldsymbol \Sigma \boldsymbol
\Sigma^\text T}{2}$ is the covariance of the component noises injected into the
$m$-th and $n$-th sensors. For different choices of $\mathbf B$ and
$\boldsymbol \Sigma,$ we investigate whether Cyclicity Analysis enables us to
recover the structure of network. Roughly speaking, Cyclicity Analysis studies
the lead-lag dynamics pertaining to the components of a multivariate signal. We
specifically consider an $N \times N$ skew-symmetric matrix $\mathbf Q,$ known
as the lead matrix, in which the sign of its $(m,n)$-th entry captures the
lead-lag relationship between the $m$-th and $n$-th component OU processes. We
investigate whether the structure of the leading eigenvector of $\mathbf Q,$
the eigenvector corresponding to the largest eigenvalue of $\mathbf Q$ in
modulus, reflects the network structure induced by $\mathbf B.$",http://arxiv.org/pdf/2409.12102v1,http://arxiv.org/abs/2409.12102v1
Symmetry-Enriched Learning: A Category-Theoretic Framework for Robust Machine Learning Models,"This manuscript presents a novel framework that integrates higher-order
symmetries and category theory into machine learning. We introduce new
mathematical constructs, including hyper-symmetry categories and functorial
representations, to model complex transformations within learning algorithms.
Our contributions include the design of symmetry-enriched learning models, the
development of advanced optimization techniques leveraging categorical
symmetries, and the theoretical analysis of their implications for model
robustness, generalization, and convergence. Through rigorous proofs and
practical applications, we demonstrate that incorporating higher-dimensional
categorical structures enhances both the theoretical foundations and practical
capabilities of modern machine learning algorithms, opening new directions for
research and innovation.",http://arxiv.org/pdf/2409.12100v1,http://arxiv.org/abs/2409.12100v1
Brain-Streams: fMRI-to-Image Reconstruction with Multi-modal Guidance,"Understanding how humans process visual information is one of the crucial
steps for unraveling the underlying mechanism of brain activity. Recently, this
curiosity has motivated the fMRI-to-image reconstruction task; given the fMRI
data from visual stimuli, it aims to reconstruct the corresponding visual
stimuli. Surprisingly, leveraging powerful generative models such as the Latent
Diffusion Model (LDM) has shown promising results in reconstructing complex
visual stimuli such as high-resolution natural images from vision datasets.
Despite the impressive structural fidelity of these reconstructions, they often
lack details of small objects, ambiguous shapes, and semantic nuances.
Consequently, the incorporation of additional semantic knowledge, beyond mere
visuals, becomes imperative. In light of this, we exploit how modern LDMs
effectively incorporate multi-modal guidance (text guidance, visual guidance,
and image layout) for structurally and semantically plausible image
generations. Specifically, inspired by the two-streams hypothesis suggesting
that perceptual and semantic information are processed in different brain
regions, our framework, Brain-Streams, maps fMRI signals from these brain
regions to appropriate embeddings. That is, by extracting textual guidance from
semantic information regions and visual guidance from perceptual information
regions, Brain-Streams provides accurate multi-modal guidance to LDMs. We
validate the reconstruction ability of Brain-Streams both quantitatively and
qualitatively on a real fMRI dataset comprising natural image stimuli and fMRI
data.",http://arxiv.org/pdf/2409.12099v1,http://arxiv.org/abs/2409.12099v1
Trading with propagators and constraints: applications to optimal execution and battery storage,"Motivated by optimal execution with stochastic signals, market impact and
constraints in financial markets, and optimal storage management in commodity
markets, we formulate and solve an optimal trading problem with a general
propagator model under linear functional inequality constraints. The optimal
control is given explicitly in terms of the corresponding Lagrange multipliers
and their conditional expectations, as a solution to a linear stochastic
Fredholm equation. We propose a stochastic version of the Uzawa algorithm on
the dual problem to construct the stochastic Lagrange multipliers numerically
via a stochastic projected gradient ascent, combined with a least-squares Monte
Carlo regression step to approximate their conditional expectations. We
illustrate our findings on two different practical applications with stochastic
signals: (i) an optimal execution problem with an exponential or a power law
decaying transient impact, with either a `no-shorting' constraint in the
presence of a `sell' signal, a `no-buying' constraint in the presence of a
`buy' signal or a stochastic `stop-trading' constraint whenever the exogenous
price drops below a specified reference level; (ii) a battery storage problem
with instantaneous operating costs, seasonal signals and fixed constraints on
both the charging power and the load capacity of the battery.",http://arxiv.org/pdf/2409.12098v1,http://arxiv.org/abs/2409.12098v1
Skill matching at scale: freelancer-project alignment for efficient multilingual candidate retrieval,"Finding the perfect match between a job proposal and a set of freelancers is
not an easy task to perform at scale, especially in multiple languages. In this
paper, we propose a novel neural retriever architecture that tackles this
problem in a multilingual setting. Our method encodes project descriptions and
freelancer profiles by leveraging pre-trained multilingual language models. The
latter are used as backbone for a custom transformer architecture that aims to
keep the structure of the profiles and project. This model is trained with a
contrastive loss on historical data. Thanks to several experiments, we show
that this approach effectively captures skill matching similarity and
facilitates efficient matching, outperforming traditional methods.",http://arxiv.org/pdf/2409.12097v2,http://arxiv.org/abs/2409.12097v2
Undersampling effects on observed periods of coronal oscillations,"Context. Recent observations of decayless transverse oscillations have shown
two branches in the relationship between periods and loop lengths. One is a
linear relationship, interpreted as a standing mode. The other shows almost no
correlation and has not yet been interpreted conclusively. Aims. We
investigated the undersampling effect on observed periods of decayless
oscillations. Methods. We considered oscillating coronal loops that closely
follow the observed loop length distribution. Assuming that all oscillations
are standing waves, we modeled a signal that represents decayless oscillations
where the period is proportional to the loop length and the amplitude and phase
are randomly drawn. A downsampled signal was generated from the original signal
by considering different sample rates that mimic temporal cadences of
telescopes, and periods for sampled signals were analysed using the fast
Fourier transform. Results. When the sampling cadence is getting closer to the
actual oscillation period, a tendency for overestimating periods in short loops
is enhanced. The relationship between loop lengths and periods of the sampled
signals shows the two branches as in the observation. Conclusions. We find that
long periods of decayless oscillations occurring in short loops could be the
result of undersampling.",http://arxiv.org/pdf/2409.12095v1,http://arxiv.org/abs/2409.12095v1
"IMRL: Integrating Visual, Physical, Temporal, and Geometric Representations for Enhanced Food Acquisition","Robotic assistive feeding holds significant promise for improving the quality
of life for individuals with eating disabilities. However, acquiring diverse
food items under varying conditions and generalizing to unseen food presents
unique challenges. Existing methods that rely on surface-level geometric
information (e.g., bounding box and pose) derived from visual cues (e.g.,
color, shape, and texture) often lacks adaptability and robustness, especially
when foods share similar physical properties but differ in visual appearance.
We employ imitation learning (IL) to learn a policy for food acquisition.
Existing methods employ IL or Reinforcement Learning (RL) to learn a policy
based on off-the-shelf image encoders such as ResNet-50. However, such
representations are not robust and struggle to generalize across diverse
acquisition scenarios. To address these limitations, we propose a novel
approach, IMRL (Integrated Multi-Dimensional Representation Learning), which
integrates visual, physical, temporal, and geometric representations to enhance
the robustness and generalizability of IL for food acquisition. Our approach
captures food types and physical properties (e.g., solid, semi-solid, granular,
liquid, and mixture), models temporal dynamics of acquisition actions, and
introduces geometric information to determine optimal scooping points and
assess bowl fullness. IMRL enables IL to adaptively adjust scooping strategies
based on context, improving the robot's capability to handle diverse food
acquisition scenarios. Experiments on a real robot demonstrate our approach's
robustness and adaptability across various foods and bowl configurations,
including zero-shot generalization to unseen settings. Our approach achieves
improvement up to $35\%$ in success rate compared with the best-performing
baseline.",http://arxiv.org/pdf/2409.12092v1,http://arxiv.org/abs/2409.12092v1
Uncovering liquid-substrate fluctuation effects on crystal growth and disordered hyperuniformity of two-dimensional materials,"We investigate the growth of two-dimensional (2D) crystals on fluctuating
surfaces using a phase field crystal model that is relevant on atomic length
and diffusive time scales. Motivated by recent experiments which achieved
unprecedented fast growth of large-size high-quality 2D crystals on liquid
substrates, we uncover novel effects of liquid surfaces on microstructural
ordering. We find that substrate fluctuations generate short-ranged noise that
speeds up crystallization and grain growth of the overlayer, surpassing that of
free-standing system. Coupling to the liquid substrate fluctuations can also
modulate local randomness, leading to intriguing disordered structures with
hidden spatial order, i.e., disordered hyperuniformity. These results reveal
the physical mechanisms underlying the fast growth of 2D crystals on liquid
surfaces and demonstrate a novel strategy for synthesizing disordered
hyperuniform thin film structures.",http://arxiv.org/pdf/2409.12090v1,http://arxiv.org/abs/2409.12090v1
The Impact of Element Ordering on LM Agent Performance,"There has been a surge of interest in language model agents that can navigate
virtual environments such as the web or desktop. To navigate such environments,
agents benefit from information on the various elements (e.g., buttons, text,
or images) present. It remains unclear which element attributes have the
greatest impact on agent performance, especially in environments that only
provide a graphical representation (i.e., pixels). Here we find that the
ordering in which elements are presented to the language model is surprisingly
impactful--randomizing element ordering in a webpage degrades agent performance
comparably to removing all visible text from an agent's state representation.
While a webpage provides a hierarchical ordering of elements, there is no such
ordering when parsing elements directly from pixels. Moreover, as tasks become
more challenging and models more sophisticated, our experiments suggest that
the impact of ordering increases. Finding an effective ordering is non-trivial.
We investigate the impact of various element ordering methods in web and
desktop environments. We find that dimensionality reduction provides a viable
ordering for pixel-only environments. We train a UI element detection model to
derive elements from pixels and apply our findings to an agent
benchmark--OmniACT--where we only have access to pixels. Our method completes
more than two times as many tasks on average relative to the previous
state-of-the-art.",http://arxiv.org/pdf/2409.12089v2,http://arxiv.org/abs/2409.12089v2
Quark saturation in the QCD phase diagram,"We determine the onset of Quarkyonic Matter corresponding to values of
temperature and baryon chemical potential at which the quark phase space
density becomes one. At zero temperature for baryon chemical potentials below
the mass of the Lambda baryon, only nucleons contribute to the quark density.
This is different at finite temperature, where all baryons, mesons and their
resonances can be excited and thus add quarks to the phase space. The
probability density to find a quark inside a hadron is determined using the
Yukawa ansatz of the IdylliQ model of Quarkyonic Matter. We estimate separately
the magnitude of the various contributions of nucleons, Delta baryons, pions as
well as further hadrons and resonances. The uncertainty in the parametrization
of the probability density to find a quark inside a nucleon is spanned by
assuming that at zero temperature the transition density to Quarkyonic Matter
is between one and three times that of nuclear matter. Various predictions for
a possible critical point associated with the chiral phase transition are found
close to a triple point at which the line of the deconfinement transition and
the curve associated with the transition to Quarkyonic Matter intersect. These
considerations provide an estimate for the region in the QCD phase diagram
where Quarkyonic Matter may be found.",http://arxiv.org/pdf/2409.12088v1,http://arxiv.org/abs/2409.12088v1
Towards Interpretable End-Stage Renal Disease (ESRD) Prediction: Utilizing Administrative Claims Data with Explainable AI Techniques,"This study explores the potential of utilizing administrative claims data,
combined with advanced machine learning and deep learning techniques, to
predict the progression of Chronic Kidney Disease (CKD) to End-Stage Renal
Disease (ESRD). We analyze a comprehensive, 10-year dataset provided by a major
health insurance organization to develop prediction models for multiple
observation windows using traditional machine learning methods such as Random
Forest and XGBoost as well as deep learning approaches such as Long Short-Term
Memory (LSTM) networks. Our findings demonstrate that the LSTM model,
particularly with a 24-month observation window, exhibits superior performance
in predicting ESRD progression, outperforming existing models in the
literature. We further apply SHapley Additive exPlanations (SHAP) analysis to
enhance interpretability, providing insights into the impact of individual
features on predictions at the individual patient level. This study underscores
the value of leveraging administrative claims data for CKD management and
predicting ESRD progression.",http://arxiv.org/pdf/2409.12087v1,http://arxiv.org/abs/2409.12087v1
Unveiling the Secrets of New Physics Through Top Quark Tagging,"The ubiquity of top-rich final states in the context of beyond the Standard
Model (BSM) searches has led to their status as extensively studied signatures
at the LHC. Over the past decade, numerous endeavours have been undertaken in
the literature to develop methods for efficiently distinguishing boosted top
quark jets from QCD jets. Although cut-based strategies for boosted top
tagging, which rely on substructure information from fat jets resulting from
the hadronic decay of boosted top quarks, were introduced in the literature as
early as 2008, recent years have witnessed a surge in the utilization of
machine learning-based approaches for the classification of top-jets from QCD
jets. The review focuses on the present status of boosted top tagging and its
application for BSM searchers.",http://arxiv.org/pdf/2409.12085v1,http://arxiv.org/abs/2409.12085v1
The $_c$-meson leading-twist distribution amplitude,"In this project, we employ the short-distance factorization to compute the
distribution amplitude of the $\eta_c$-meson from Lattice QCD at leading twist.
We employ a set of CLS $N_f=2$ ensembles at three lattice spacings and various
quark masses to extrapolate the pseudo distribution to the physical point in
the isospin limit. We solve the inverse problem modeling the distribution
amplitude, and we match our results to the light-cone in the
$\overline{\text{MS}}$-scheme. We include a complete error budget, and we
compare to two alternative approaches: non-relativistic QCD and Dyson-Schwinger
equations, finding good agreement with the latter but not with the former.",http://arxiv.org/pdf/2409.12084v1,http://arxiv.org/abs/2409.12084v1
ReLU Surrogates in Mixed-Integer MPC for Irrigation Scheduling,"Efficient water management in agriculture is important for mitigating the
growing freshwater scarcity crisis. Mixed-integer Model Predictive Control
(MPC) has emerged as an effective approach for addressing the complex
scheduling problems in agricultural irrigation. However, the computational
complexity of mixed-integer MPC still poses a significant challenge,
particularly in large-scale applications. This study proposes an approach to
enhance the computational efficiency of mixed-integer MPC-based irrigation
schedulers by employing ReLU surrogate models to describe the soil moisture
dynamics of the agricultural field. By leveraging the mixed-integer linear
representation of the ReLU operator, the proposed approach transforms the
mixed-integer MPC-based scheduler with a quadratic cost function into a
mixed-integer quadratic program, which is the simplest class of mixed-integer
nonlinear programming problems that can be efficiently solved using global
optimization solvers. The effectiveness of this approach is demonstrated
through comparative studies conducted on a large-scale agricultural field
across two growing seasons, involving other machine learning surrogate models,
specifically Long Short-Term Memory (LSTM) networks, and the widely used
triggered irrigation scheduling method. The ReLU-based approach significantly
reduces solution times -- by up to 99.5\% -- while achieving comparable
performance to the LSTM approach in terms of water savings and Irrigation Water
Use Efficiency (IWUE). Moreover, the ReLU-based approach maintains enhanced
performance in terms of total prescribed irrigation and IWUE compared to the
widely-used triggered irrigation scheduling method.",http://arxiv.org/pdf/2409.12082v1,http://arxiv.org/abs/2409.12082v1
Design of Ligand-Binding Proteins with Atomic Flow Matching,"Designing novel proteins that bind to small molecules is a long-standing
challenge in computational biology, with applications in developing catalysts,
biosensors, and more. Current computational methods rely on the assumption that
the binding pose of the target molecule is known, which is not always feasible,
as conformations of novel targets are often unknown and tend to change upon
binding. In this work, we formulate proteins and molecules as unified
biotokens, and present AtomFlow, a novel deep generative model under the
flow-matching framework for the design of ligand-binding proteins from the 2D
target molecular graph alone. Operating on representative atoms of biotokens,
AtomFlow captures the flexibility of ligands and generates ligand conformations
and protein backbone structures iteratively. We consider the multi-scale nature
of biotokens and demonstrate that AtomFlow can be effectively trained on a
subset of structures from the Protein Data Bank, by matching flow vector field
using an SE(3) equivariant structure prediction network. Experimental results
show that our method can generate high fidelity ligand-binding proteins and
achieve performance comparable to the state-of-the-art model RFDiffusionAA,
while not requiring bound ligand structures. As a general framework, AtomFlow
holds the potential to be applied to various biomolecule generation tasks in
the future.",http://arxiv.org/pdf/2409.12080v1,http://arxiv.org/abs/2409.12080v1
Denoising diffusion models for high-resolution microscopy image restoration,"Advances in microscopy imaging enable researchers to visualize structures at
the nanoscale level thereby unraveling intricate details of biological
organization. However, challenges such as image noise, photobleaching of
fluorophores, and low tolerability of biological samples to high light doses
remain, restricting temporal resolutions and experiment durations. Reduced
laser doses enable longer measurements at the cost of lower resolution and
increased noise, which hinders accurate downstream analyses. Here we train a
denoising diffusion probabilistic model (DDPM) to predict high-resolution
images by conditioning the model on low-resolution information. Additionally,
the probabilistic aspect of the DDPM allows for repeated generation of images
that tend to further increase the signal-to-noise ratio. We show that our model
achieves a performance that is better or similar to the previously
best-performing methods, across four highly diverse datasets. Importantly,
while any of the previous methods show competitive performance for some, but
not all datasets, our method consistently achieves high performance across all
four data sets, suggesting high generalizability.",http://arxiv.org/pdf/2409.12078v1,http://arxiv.org/abs/2409.12078v1
Unsupervised Domain Adaptation Via Data Pruning,"The removal of carefully-selected examples from training data has recently
emerged as an effective way of improving the robustness of machine learning
models. However, the best way to select these examples remains an open
question. In this paper, we consider the problem from the perspective of
unsupervised domain adaptation (UDA). We propose AdaPrune, a method for UDA
whereby training examples are removed to attempt to align the training
distribution to that of the target data. By adopting the maximum mean
discrepancy (MMD) as the criterion for alignment, the problem can be neatly
formulated and solved as an integer quadratic program. We evaluate our approach
on a real-world domain shift task of bioacoustic event detection. As a method
for UDA, we show that AdaPrune outperforms related techniques, and is
complementary to other UDA algorithms such as CORAL. Our analysis of the
relationship between the MMD and model accuracy, along with t-SNE plots,
validate the proposed method as a principled and well-founded way of performing
data pruning.",http://arxiv.org/pdf/2409.12076v1,http://arxiv.org/abs/2409.12076v1
Online Refractive Camera Model Calibration in Visual Inertial Odometry,"This paper presents a general refractive camera model and online
co-estimation of odometry and the refractive index of unknown media. This
enables operation in diverse and varying refractive fluids, given only the
camera calibration in air. The refractive index is estimated online as a state
variable of a monocular visual-inertial odometry framework in an iterative
formulation using the proposed camera model. The method was verified on data
collected using an underwater robot traversing inside a pool. The evaluations
demonstrate convergence to the ideal refractive index for water despite
significant perturbations in the initialization. Simultaneously, the approach
enables on-par visual-inertial odometry performance in refractive media without
prior knowledge of the refractive index or requirement of medium-specific
camera calibration.",http://arxiv.org/pdf/2409.12074v1,http://arxiv.org/abs/2409.12074v1
Influence of dislocations in multilayer graphene stacks: A phase field crystal study,"In this work the influence of $5|7$ dislocations in multiplayer graphene
stacks (up to six layers) is examined. The study is conducted through a
recently developed Phase Field Crystal (PFC) model for multilayer systems
incorporating out-of-plane deformations and parameterized to match to density
functional theory calculations for graphene bilayers and other systems. The
specific configuration considered consists of one monolayer containing four
$5|7$ dislocations (i.e., two dislocation dipoles) sandwiched in between
perfect graphene layers. The study reveals how the strain field from the
dislocations in the defected layer leads to out-of-plane deformations that in
turn cause deformations of neighboring layers. Quantitative predictions are
made for the defect free energy of the multilayer stacks as compared to a
defect-free system, which is shown to increase with the number of layers and
system size. Furthermore it is predicted that system defect energy saturates by
roughly ten sheets in the stack, indicating the range of defect influence
across the multilayer. Variations of stress field distribution and layer height
profiles in different layer of the stack are also quantitatively identified.",http://arxiv.org/pdf/2409.12073v1,http://arxiv.org/abs/2409.12073v1
PAD-FT: A Lightweight Defense for Backdoor Attacks via Data Purification and Fine-Tuning,"Backdoor attacks pose a significant threat to deep neural networks,
particularly as recent advancements have led to increasingly subtle
implantation, making the defense more challenging. Existing defense mechanisms
typically rely on an additional clean dataset as a standard reference and
involve retraining an auxiliary model or fine-tuning the entire victim model.
However, these approaches are often computationally expensive and not always
feasible in practical applications. In this paper, we propose a novel and
lightweight defense mechanism, termed PAD-FT, that does not require an
additional clean dataset and fine-tunes only a very small part of the model to
disinfect the victim model. To achieve this, our approach first introduces a
simple data purification process to identify and select the most-likely clean
data from the poisoned training dataset. The self-purified clean dataset is
then used for activation clipping and fine-tuning only the last classification
layer of the victim model. By integrating data purification, activation
clipping, and classifier fine-tuning, our mechanism PAD-FT demonstrates
superior effectiveness across multiple backdoor attack methods and datasets, as
confirmed through extensive experimental evaluation.",http://arxiv.org/pdf/2409.12072v1,http://arxiv.org/abs/2409.12072v1
Fitting Multilevel Factor Models,"We examine a special case of the multilevel factor model, with covariance
given by multilevel low rank (MLR) matrix~\cite{parshakova2023factor}. We
develop a novel, fast implementation of the expectation-maximization (EM)
algorithm, tailored for multilevel factor models, to maximize the likelihood of
the observed data. This method accommodates any hierarchical structure and
maintains linear time and storage complexities per iteration. This is achieved
through a new efficient technique for computing the inverse of the positive
definite MLR matrix. We show that the inverse of an invertible PSD MLR matrix
is also an MLR matrix with the same sparsity in factors, and we use the
recursive Sherman-Morrison-Woodbury matrix identity to obtain the factors of
the inverse. Additionally, we present an algorithm that computes the Cholesky
factorization of an expanded matrix with linear time and space complexities,
yielding the covariance matrix as its Schur complement. This paper is
accompanied by an open-source package that implements the proposed methods.",http://arxiv.org/pdf/2409.12067v1,http://arxiv.org/abs/2409.12067v1
Drifts of the sub-stellar points of the TRAPPIST-1 planets,"Accurate modeling of tidal interactions is crucial for interpreting recent
JWST observations of the thermal emissions of TRAPPIST-1~b and c and for
characterizing the surface conditions and potential habitability of the other
planets in the system. Indeed, the rotation state of the planets, driven by
tidal forces, significantly influences the heat redistribution regime. Due to
their proximity to their host star and the estimated age of the system, the
TRAPPIST-1 planets are commonly assumed to be in a synchronization state. In
this work, we present the recent implementation of the co-planar tidal torque
and forces equations within the formalism of Kaula in the N-body code
Posidonius. This enables us to explore the hypothesis of synchronization using
a tidal model well suited to rocky planets. We studied the rotational state of
each planet by taking into account their multi-layer internal structure
computed with the code Burnman. Simulations show that the TRAPPIST-1 planets
are not perfectly synchronized but oscillate around the synchronization state.
Planet-planet interactions lead to strong variations on the mean motion and
tides fail to keep the spin synchronized with respect to the mean motion. As a
result, the sub-stellar point of each planet experiences short oscillations and
long-timescale drifts that lead the planets to achieve a synodic day with
periods varying from $55$~years to $290$~years depending on the planet.",http://arxiv.org/pdf/2409.12065v1,http://arxiv.org/abs/2409.12065v1
Quantum Magic and Multi-Partite Entanglement in the Structure of Nuclei,"Motivated by the Gottesman-Knill theorem, we present a detailed study of the
quantum complexity of $p$-shell and $sd$-shell nuclei. Valence-space nuclear
shell-model wavefunctions generated by the BIGSTICK code are mapped to qubit
registers using the Jordan-Wigner mapping (12 qubits for the $p$-shell and 24
qubits for the $sd$-shell), from which measures of the many-body entanglement
($n$-tangles) and magic (non-stabilizerness) are determined. While exact
evaluations of these measures are possible for nuclei with a modest number of
active nucleons, Monte Carlo simulations are required for the more complex
nuclei. The broadly-applicable Pauli-String $IZ$ exact (PSIZe-) MCMC technique
is introduced to accelerate the evaluation of measures of magic in deformed
nuclei (with hierarchical wavefunctions), by factors of $\sim 8$ for some
nuclei. Significant multi-nucleon entanglement is found in the $sd$-shell,
dominated by proton-neutron configurations, along with significant measures of
magic. This is evident not only for the deformed states, but also for nuclei on
the path to instability via regions of shape coexistence and level inversion.
These results indicate that quantum-computing resources will accelerate
precision simulations of such nuclei and beyond.",http://arxiv.org/pdf/2409.12064v1,http://arxiv.org/abs/2409.12064v1
"Post-Keplerian perturbations of the hyperbolic motion in the field of a massive, rotating object","The perturbations of the hyperbolic motion of a test particle due to the
general relativistic gravitoelectromagnetic Schwarzschild and Lense-Thirring
components of the gravitational field of a massive, rotating body are
analytically worked out to the first post-Newtonian level. To the Newtonian
order, the impact of the quadrupole mass moment of the source is calculated as
well. The resulting analytical expressions are valid for a generic orientation
in space of both the orbital plane of the probe and the spin axis of the
primary, and for arbitrary values of the eccentricity. They are applied first
to 'Oumuamua, an interstellar asteroid which recently visited our solar system
along an unbound heliocentric orbit. While its gravitoelectric shifts occurred
close to the Sun's flyby are less than some tens of milliarcseconds, those due
to the solar oblateness and angular momentum are of the order of
microarcseconds throughout the whole trajectory. Comparable values occur for
the post-Newtonian shifts of the Near Earth Asteroid Rendezvous (NEAR)
spacecraft during its flyby of the Earth, while those due to the oblateness of
the latter are nominally several orders of magnitude larger. The current
(formal) uncertainty in the quadrupole mass moment of the geopotential would
bring the mismodeling of such classical effects below the nominal value of the
predicted relativistic disturbances. The hyperbolic excess velocity is not
changed by any of the post--Keplerian accelerations considered. The
calculational approach developed can be straightforwardly extended to any
alternative models of gravity as well.",http://arxiv.org/pdf/2409.12063v1,http://arxiv.org/abs/2409.12063v1
Generalized Robot Learning Framework,"Imitation based robot learning has recently gained significant attention in
the robotics field due to its theoretical potential for transferability and
generalizability. However, it remains notoriously costly, both in terms of
hardware and data collection, and deploying it in real-world environments
demands meticulous setup of robots and precise experimental conditions. In this
paper, we present a low-cost robot learning framework that is both easily
reproducible and transferable to various robots and environments. We
demonstrate that deployable imitation learning can be successfully applied even
to industrial-grade robots, not just expensive collaborative robotic arms.
Furthermore, our results show that multi-task robot learning is achievable with
simple network architectures and fewer demonstrations than previously thought
necessary. As the current evaluating method is almost subjective when it comes
to real-world manipulation tasks, we propose Voting Positive Rate (VPR) - a
novel evaluation strategy that provides a more objective assessment of
performance. We conduct an extensive comparison of success rates across various
self-designed tasks to validate our approach. To foster collaboration and
support the robot learning community, we have open-sourced all relevant
datasets and model checkpoints, available at huggingface.co/ZhiChengAI.",http://arxiv.org/pdf/2409.12061v1,http://arxiv.org/abs/2409.12061v1
PARAPHRASUS : A Comprehensive Benchmark for Evaluating Paraphrase Detection Models,"The task of determining whether two texts are paraphrases has long been a
challenge in NLP. However, the prevailing notion of paraphrase is often quite
simplistic, offering only a limited view of the vast spectrum of paraphrase
phenomena. Indeed, we find that evaluating models in a paraphrase dataset can
leave uncertainty about their true semantic understanding. To alleviate this,
we release paraphrasus, a benchmark designed for multi-dimensional assessment
of paraphrase detection models and finer model selection. We find that
paraphrase detection models under a fine-grained evaluation lens exhibit
trade-offs that cannot be captured through a single classification dataset.",http://arxiv.org/pdf/2409.12060v1,http://arxiv.org/abs/2409.12060v1
Dual-Layer Training and Decoding of Large Language Model with Simultaneously Thinking and Speaking,"Large Language Model can reasonably understand and generate human expressions
but may lack of thorough thinking and reasoning mechanisms. Recently there have
been several studies which enhance the thinking ability of language models but
most of them are not data-driven or training-based. In this paper, we are
motivated by the cognitive mechanism in the natural world, and design a novel
model architecture called TaS which allows it to first consider the thoughts
and then express the response based upon the query. We design several pipelines
to annotate or generate the thought contents from prompt-response samples, then
add language heads in a middle layer which behaves as the thinking layer. We
train the language model by the thoughts-augmented data and successfully let
the thinking layer automatically generate reasonable thoughts and finally
output more reasonable responses. Both qualitative examples and quantitative
results validate the effectiveness and performance of TaS. Our code is
available at https://anonymous.4open.science/r/TadE.",http://arxiv.org/pdf/2409.12059v1,http://arxiv.org/abs/2409.12059v1
Artemis: Efficient Commit-and-Prove SNARKs for zkML,"The widespread adoption of machine learning (ML) in various critical
applications, from healthcare to autonomous systems, has raised significant
concerns about privacy, accountability, and trustworthiness. To address these
concerns, recent research has focused on developing zero-knowledge machine
learning (zkML) techniques that enable the verification of various aspects of
ML models without revealing sensitive information. Recent advances in zkML have
substantially improved efficiency; however, these efforts have primarily
optimized the process of proving ML computations correct, often overlooking the
substantial overhead associated with verifying the necessary commitments to the
model and data. To address this gap, this paper introduces two new
Commit-and-Prove SNARK (CP-SNARK) constructions (Apollo and Artemis) that
effectively address the emerging challenge of commitment verification in zkML
pipelines. Apollo operates on KZG commitments and requires white-box use of the
underlying proof system, whereas Artemis is compatible with any homomorphic
polynomial commitment and only makes black-box use of the proof system. As a
result, Artemis is compatible with state-of-the-art proof systems without
trusted setup. We present the first implementation of these CP-SNARKs, evaluate
their performance on a diverse set of ML models, and show substantial
improvements over existing methods, achieving significant reductions in prover
costs and maintaining efficiency even for large-scale models. For example, for
the VGG model, we reduce the overhead associated with commitment checks from
11.5x to 1.2x. Our results suggest that these contributions can move zkML
towards practical deployment, particularly in scenarios involving large and
complex ML models.",http://arxiv.org/pdf/2409.12055v1,http://arxiv.org/abs/2409.12055v1
MMP for Enriques pairs and singular Enriques varieties,"We introduce and study the class of primitive Enriques varieties, whose
smooth members are Enriques manifolds. We provide several examples and we
demonstrate that this class is stable under the operations of the Minimal Model
Program (MMP). In particular, given an Enriques manifold $Y$ and an effective
$\mathbb{R}$-divisor $B_Y$ on $Y$ such that the pair $(Y,B_Y)$ is log
canonical, we prove that any $(K_Y+B_Y)$-MMP terminates with a minimal model
$(Y',B_{Y'})$ of $(Y,B_Y)$, where $Y'$ is a $\mathbb{Q}$-factorial primitive
Enriques variety with canonical singularities. Finally, we investigate the
asymptotic theory of Enriques manifolds.",http://arxiv.org/pdf/2409.12054v1,http://arxiv.org/abs/2409.12054v1
Numerical renormalization group calculations for magnetic impurity systems with spin-orbit coupling and crystal-field effects,"Exploiting symmetries in the numerical renormalization group (NRG) method
significantly enhances performance by improving accuracy, increasing
computational speed, and optimizing memory efficiency. Published codes focus on
continuous rotations and unitary groups, which generally are not applicable to
systems with strong crystal-field effects. The PointGroupNRG code implements
symmetries related to discrete rotation groups, which are defined by the user
in terms of Clebsch-Gordan coefficients, together with particle conservation
and spin rotation symmetries. In this paper we present a new version of the
code that extends the available finite groups, previously limited to simply
reducible point groups, in a way that all point and double groups become
accessible. It also includes the full spin-orbital rotation group. Moreover, to
improve the code's flexibility for impurities with complex interactions, this
new version allows to choose between a standard Anderson Hamiltonian for the
impurity or, as another novel feature, an ionic model that requires only the
spectrum and the impurity Lehmann amplitudes.",http://arxiv.org/pdf/2409.12050v1,http://arxiv.org/abs/2409.12050v1
A Survey-Based Quantitative Analysis of Stress Factors and Their Impacts Among Cybersecurity Professionals,"This study investigates the prevalence and underlying causes of work-related
stress and burnout among cybersecurity professionals using a quantitative
survey approach guided by the Job Demands-Resources model. Analysis of
responses from 50 cybersecurity practitioners reveals an alarming reality: 44%
report experiencing severe work-related stress and burnout, while an additional
28% are uncertain about their condition. The demanding nature of cybersecurity
roles, unrealistic expectations, and unsupportive organizational cultures
emerge as primary factors fueling this crisis. Notably, 66% of respondents
perceive cybersecurity jobs as more stressful than other IT positions, with 84%
facing additional challenges due to the pandemic and recent high-profile
breaches. The study finds that most cybersecurity experts are reluctant to
report their struggles to management, perpetuating a cycle of silence and
neglect. To address this critical issue, the paper recommends that
organizations foster supportive work environments, implement mindfulness
programs, and address systemic challenges. By prioritizing the mental health of
cybersecurity professionals, organizations can cultivate a more resilient and
effective workforce to protect against an ever-evolving threat landscape.",http://arxiv.org/pdf/2409.12047v1,http://arxiv.org/abs/2409.12047v1
Using Large Language Models to Generate Clinical Trial Tables and Figures,"Tables, figures, and listings (TFLs) are essential tools for summarizing
clinical trial data. Creation of TFLs for reporting activities is often a
time-consuming task encountered routinely during the execution of clinical
trials. This study explored the use of large language models (LLMs) to automate
the generation of TFLs through prompt engineering and few-shot transfer
learning. Using public clinical trial data in ADaM format, our results
demonstrated that LLMs can efficiently generate TFLs with prompt instructions,
showcasing their potential in this domain. Furthermore, we developed a
conservational agent named Clinical Trial TFL Generation Agent: An app that
matches user queries to predefined prompts that produce customized programs to
generate specific predefined TFLs.",http://arxiv.org/pdf/2409.12046v2,http://arxiv.org/abs/2409.12046v2
Handling Long-Term Safety and Uncertainty in Safe Reinforcement Learning,"Safety is one of the key issues preventing the deployment of reinforcement
learning techniques in real-world robots. While most approaches in the Safe
Reinforcement Learning area do not require prior knowledge of constraints and
robot kinematics and rely solely on data, it is often difficult to deploy them
in complex real-world settings. Instead, model-based approaches that
incorporate prior knowledge of the constraints and dynamics into the learning
framework have proven capable of deploying the learning algorithm directly on
the real robot. Unfortunately, while an approximated model of the robot
dynamics is often available, the safety constraints are task-specific and hard
to obtain: they may be too complicated to encode analytically, too expensive to
compute, or it may be difficult to envision a priori the long-term safety
requirements. In this paper, we bridge this gap by extending the safe
exploration method, ATACOM, with learnable constraints, with a particular focus
on ensuring long-term safety and handling of uncertainty. Our approach is
competitive or superior to state-of-the-art methods in final performance while
maintaining safer behavior during training.",http://arxiv.org/pdf/2409.12045v1,http://arxiv.org/abs/2409.12045v1
Quasiperiodic Floquet-Gibbs states in Rydberg atomic systems,"Open systems that are weakly coupled to a thermal environment and driven by
fast, periodically oscillating fields are commonly assumed to approach an
equilibrium-like steady state with respect to a truncated Floquet-Magnus
Hamiltonian. Using a general argument based on Fermi's golden rule, we show
that such Floquet-Gibbs states emerge naturally in periodically modulated
Rydberg atomic systems, whose lab-frame Hamiltonian is a quasiperiodic function
of time. Our approach applies as long as the inherent Bohr frequencies of the
system, the modulation frequency and the frequency of the driving laser, which
is necessary to uphold high-lying Rydberg excitations, are well separated. To
corroborate our analytical results, we analyze a realistic model of up to five
interacting Rydberg atoms with periodically changing detuning. We demonstrate
numerically that the second-order Floquet-Gibbs state of this system is
essentially indistinguishable from the steady state of the corresponding
Redfield equation if the modulation and driving frequencies are sufficiently
large.",http://arxiv.org/pdf/2409.12044v1,http://arxiv.org/abs/2409.12044v1
Understanding the Effects of the Baidu-ULTR Logging Policy on Two-Tower Models,"Despite the popularity of the two-tower model for unbiased learning to rank
(ULTR) tasks, recent work suggests that it suffers from a major limitation that
could lead to its collapse in industry applications: the problem of logging
policy confounding. Several potential solutions have even been proposed;
however, the evaluation of these methods was mostly conducted using
semi-synthetic simulation experiments. This paper bridges the gap between
theory and practice by investigating the confounding problem on the largest
real-world dataset, Baidu-ULTR. Our main contributions are threefold: 1) we
show that the conditions for the confounding problem are given on Baidu-ULTR,
2) the confounding problem bears no significant effect on the two-tower model,
and 3) we point to a potential mismatch between expert annotations, the golden
standard in ULTR, and user click behavior.",http://arxiv.org/pdf/2409.12043v1,http://arxiv.org/abs/2409.12043v1
ASR Benchmarking: Need for a More Representative Conversational Dataset,"Automatic Speech Recognition (ASR) systems have achieved remarkable
performance on widely used benchmarks such as LibriSpeech and Fleurs. However,
these benchmarks do not adequately reflect the complexities of real-world
conversational environments, where speech is often unstructured and contains
disfluencies such as pauses, interruptions, and diverse accents. In this study,
we introduce a multilingual conversational dataset, derived from TalkBank,
consisting of unstructured phone conversation between adults. Our results show
a significant performance drop across various state-of-the-art ASR models when
tested in conversational settings. Furthermore, we observe a correlation
between Word Error Rate and the presence of speech disfluencies, highlighting
the critical need for more realistic, conversational ASR benchmarks.",http://arxiv.org/pdf/2409.12042v1,http://arxiv.org/abs/2409.12042v1
SFDA-rPPG: Source-Free Domain Adaptive Remote Physiological Measurement with Spatio-Temporal Consistency,"Remote Photoplethysmography (rPPG) is a non-contact method that uses facial
video to predict changes in blood volume, enabling physiological metrics
measurement. Traditional rPPG models often struggle with poor generalization
capacity in unseen domains. Current solutions to this problem is to improve its
generalization in the target domain through Domain Generalization (DG) or
Domain Adaptation (DA). However, both traditional methods require access to
both source domain data and target domain data, which cannot be implemented in
scenarios with limited access to source data, and another issue is the privacy
of accessing source domain data. In this paper, we propose the first
Source-free Domain Adaptation benchmark for rPPG measurement (SFDA-rPPG), which
overcomes these limitations by enabling effective domain adaptation without
access to source domain data. Our framework incorporates a Three-Branch
Spatio-Temporal Consistency Network (TSTC-Net) to enhance feature consistency
across domains. Furthermore, we propose a new rPPG distribution alignment loss
based on the Frequency-domain Wasserstein Distance (FWD), which leverages
optimal transport to align power spectrum distributions across domains
effectively and further enforces the alignment of the three branches. Extensive
cross-domain experiments and ablation studies demonstrate the effectiveness of
our proposed method in source-free domain adaptation settings. Our findings
highlight the significant contribution of the proposed FWD loss for
distributional alignment, providing a valuable reference for future research
and applications. The source code is available at
https://github.com/XieYiping66/SFDA-rPPG",http://arxiv.org/pdf/2409.12040v1,http://arxiv.org/abs/2409.12040v1
Not-so-glass-like Caging and Fluctuations of an Active Matter Model,"Simple active models of matter recapitulate complex biological phenomena.
Their out-of-equilibrium nature, however, often makes these models beyond the
reach of first-principle descriptions. This limitation is particularly
perplexing when attempting to distinguish between different glass-forming
mechanisms. We here consider a minimal active system in various spatial
dimensions to identify the underlying processes at play. Activity is found to
markedly impact cage escape processes and critical fluctuations, thus offering
a distinct perspective on the role of activity sluggish dynamics.",http://arxiv.org/pdf/2409.12037v1,http://arxiv.org/abs/2409.12037v1
Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes,"Graph Neural Networks based on the message-passing (MP) mechanism are a
dominant approach for handling graph-structured data. However, they are
inherently limited to modeling only pairwise interactions, making it difficult
to explicitly capture the complexity of systems with $n$-body relations. To
address this, topological deep learning has emerged as a promising field for
studying and modeling higher-order interactions using various topological
domains, such as simplicial and cellular complexes. While these new domains
provide powerful representations, they introduce new challenges, such as
effectively modeling the interactions among higher-order structures through
higher-order MP. Meanwhile, structured state-space sequence models have proven
to be effective for sequence modeling and have recently been adapted for graph
data by encoding the neighborhood of a node as a sequence, thereby avoiding the
MP mechanism. In this work, we propose a novel architecture designed to operate
with simplicial complexes, utilizing the Mamba state-space model as its
backbone. Our approach generates sequences for the nodes based on the
neighboring cells, enabling direct communication between all higher-order
structures, regardless of their rank. We extensively validate our model,
demonstrating that it achieves competitive performance compared to
state-of-the-art models developed for simplicial complexes.",http://arxiv.org/pdf/2409.12033v1,http://arxiv.org/abs/2409.12033v1
PhysMamba: Efficient Remote Physiological Measurement with SlowFast Temporal Difference Mamba,"Facial-video based Remote photoplethysmography (rPPG) aims at measuring
physiological signals and monitoring heart activity without any contact,
showing significant potential in various applications. Previous deep learning
based rPPG measurement are primarily based on CNNs and Transformers. However,
the limited receptive fields of CNNs restrict their ability to capture
long-range spatio-temporal dependencies, while Transformers also struggle with
modeling long video sequences with high complexity. Recently, the state space
models (SSMs) represented by Mamba are known for their impressive performance
on capturing long-range dependencies from long sequences. In this paper, we
propose the PhysMamba, a Mamba-based framework, to efficiently represent
long-range physiological dependencies from facial videos. Specifically, we
introduce the Temporal Difference Mamba block to first enhance local dynamic
differences and further model the long-range spatio-temporal context. Moreover,
a dual-stream SlowFast architecture is utilized to fuse the multi-scale
temporal features. Extensive experiments are conducted on three benchmark
datasets to demonstrate the superiority and efficiency of PhysMamba. The codes
are available at https://github.com/Chaoqi31/PhysMamba",http://arxiv.org/pdf/2409.12031v1,http://arxiv.org/abs/2409.12031v1
Interlayer dislocations in multilayer and bulk MoS${}_2$,"Dislocations in van der Waals materials are linear defects confined to the
interfaces between consecutive stoichiometric monolayers of a bulk layered
crystal. Here, we present a mesoscale model for the description of interlayer
dislocations in thin films of transition metal dichalcogenides. Taking
2H-MoS${}_2$ as a representative material, we compute the dependence of the
dislocation energy on the film thickness, from few-layer MoS$_2$ to the bulk
crystal, and analyse the strain field in the layers surrounding a dislocation.
We also analyse the influence of strain field on the band edge profiles for
electrons and holes, and conclude that the resulting energy profiles are
incapable of localising charge carriers, in particular at room temperature.",http://arxiv.org/pdf/2409.12030v1,http://arxiv.org/abs/2409.12030v1
