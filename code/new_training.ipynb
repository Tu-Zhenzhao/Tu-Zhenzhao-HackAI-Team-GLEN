{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff2c217e-5dfc-4180-b2a5-a16ff7ae582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42724a3f-29ec-4709-a0cf-daddb947510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test see if GPU is ready\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA is ready!\")\n",
    "        device = torch.cuda.get_device_name(0)\n",
    "        print(f\"{device} is ready!\")\n",
    "    else:\n",
    "        print(\"CUDA is gone...\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ec04c2d-784b-4b15-8f7f-64fef138b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is ready!\n",
      "NVIDIA GeForce RTX 4090 is ready!\n"
     ]
    }
   ],
   "source": [
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3047ec9d-a39e-4104-bbe4-aa5493d849f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model info\n",
    "base_model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#dataset_name = \"arxiv_papers\"\n",
    "new_model = \"/project/models/NV-llama3.1-8b-Arxiv\"\n",
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ffa6bf-2ac4-4d2c-8d82-6f170695dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv(\"ml_papers.csv\")\n",
    "data = data.dropna(subset=['title', 'summary']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ac0be7a-532b-42f0-8ccf-082a99d4ec50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>arxiv_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gender Representation and Bias in Indian Civil...</td>\n",
       "      <td>This paper makes three key contributions. Firs...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12194v2</td>\n",
       "      <td>http://arxiv.org/abs/2409.12194v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DynaMo: In-Domain Dynamics Pretraining for Vis...</td>\n",
       "      <td>Imitation learning has proven to be a powerful...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12192v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12192v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2-VL: Enhancing Vision-Language Model's Pe...</td>\n",
       "      <td>We present the Qwen2-VL Series, an advanced up...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12191v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12191v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Massively Multi-Person 3D Human Motion Forecas...</td>\n",
       "      <td>Forecasting long-term 3D human motion is chall...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12189v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12189v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPECTER: An Instrument Concept for CMB Spectra...</td>\n",
       "      <td>Deviations of the cosmic microwave background ...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12188v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12188v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SFDA-rPPG: Source-Free Domain Adaptive Remote ...</td>\n",
       "      <td>Remote Photoplethysmography (rPPG) is a non-co...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12040v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12040v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Not-so-glass-like Caging and Fluctuations of a...</td>\n",
       "      <td>Simple active models of matter recapitulate co...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12037v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12037v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Topological Deep Learning with State-Space Mod...</td>\n",
       "      <td>Graph Neural Networks based on the message-pas...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12033v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12033v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>PhysMamba: Efficient Remote Physiological Meas...</td>\n",
       "      <td>Facial-video based Remote photoplethysmography...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12031v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12031v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Interlayer dislocations in multilayer and bulk...</td>\n",
       "      <td>Dislocations in van der Waals materials are li...</td>\n",
       "      <td>http://arxiv.org/pdf/2409.12030v1</td>\n",
       "      <td>http://arxiv.org/abs/2409.12030v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Gender Representation and Bias in Indian Civil...   \n",
       "1   DynaMo: In-Domain Dynamics Pretraining for Vis...   \n",
       "2   Qwen2-VL: Enhancing Vision-Language Model's Pe...   \n",
       "3   Massively Multi-Person 3D Human Motion Forecas...   \n",
       "4   SPECTER: An Instrument Concept for CMB Spectra...   \n",
       "..                                                ...   \n",
       "75  SFDA-rPPG: Source-Free Domain Adaptive Remote ...   \n",
       "76  Not-so-glass-like Caging and Fluctuations of a...   \n",
       "77  Topological Deep Learning with State-Space Mod...   \n",
       "78  PhysMamba: Efficient Remote Physiological Meas...   \n",
       "79  Interlayer dislocations in multilayer and bulk...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   This paper makes three key contributions. Firs...   \n",
       "1   Imitation learning has proven to be a powerful...   \n",
       "2   We present the Qwen2-VL Series, an advanced up...   \n",
       "3   Forecasting long-term 3D human motion is chall...   \n",
       "4   Deviations of the cosmic microwave background ...   \n",
       "..                                                ...   \n",
       "75  Remote Photoplethysmography (rPPG) is a non-co...   \n",
       "76  Simple active models of matter recapitulate co...   \n",
       "77  Graph Neural Networks based on the message-pas...   \n",
       "78  Facial-video based Remote photoplethysmography...   \n",
       "79  Dislocations in van der Waals materials are li...   \n",
       "\n",
       "                              pdf_url                         arxiv_link  \n",
       "0   http://arxiv.org/pdf/2409.12194v2  http://arxiv.org/abs/2409.12194v2  \n",
       "1   http://arxiv.org/pdf/2409.12192v1  http://arxiv.org/abs/2409.12192v1  \n",
       "2   http://arxiv.org/pdf/2409.12191v1  http://arxiv.org/abs/2409.12191v1  \n",
       "3   http://arxiv.org/pdf/2409.12189v1  http://arxiv.org/abs/2409.12189v1  \n",
       "4   http://arxiv.org/pdf/2409.12188v1  http://arxiv.org/abs/2409.12188v1  \n",
       "..                                ...                                ...  \n",
       "75  http://arxiv.org/pdf/2409.12040v1  http://arxiv.org/abs/2409.12040v1  \n",
       "76  http://arxiv.org/pdf/2409.12037v1  http://arxiv.org/abs/2409.12037v1  \n",
       "77  http://arxiv.org/pdf/2409.12033v1  http://arxiv.org/abs/2409.12033v1  \n",
       "78  http://arxiv.org/pdf/2409.12031v1  http://arxiv.org/abs/2409.12031v1  \n",
       "79  http://arxiv.org/pdf/2409.12030v1  http://arxiv.org/abs/2409.12030v1  \n",
       "\n",
       "[80 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ddbe8cb-312c-48c6-bbde-ddc9fb000058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract topics from titles\n",
    "def extract_topic(title):\n",
    "    title = re.sub(r\"\\(.*?\\)|\\[.*?\\]\", \"\", title)\n",
    "    title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    title = title.lower()\n",
    "    return title.strip()\n",
    "\n",
    "# Generate user queries\n",
    "def generate_user_query(topic):\n",
    "    return f\"I'm looking for papers discussing {topic}.\"\n",
    "\n",
    "# Create assistant responses\n",
    "def create_assistant_response(row):\n",
    "    title = row['title']\n",
    "    summary = row['summary']\n",
    "    response = f\"One paper that discusses this topic is '{title}'. {summary}\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "498f76dd-bc73-467c-9359-fad4e4f47d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic'] = data['title'].apply(extract_topic)\n",
    "data['instruction'] = data['topic'].apply(generate_user_query)\n",
    "\n",
    "# generate assistant responses\n",
    "data['response'] = data.apply(create_assistant_response, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "425f9e63-5732-43e8-b12d-11d67495b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One paper that discusses this topic is 'DynaMo: In-Domain Dynamics Pretraining for Visuo-Motor Control'. Imitation learning has proven to be a powerful tool for training complex\\nvisuomotor policies. However, current methods often require hundreds to\\nthousands of expert demonstrations to handle high-dimensional visual\\nobservations. A key reason for this poor data efficiency is that visual\\nrepresentations are predominantly either pretrained on out-of-domain data or\\ntrained directly through a behavior cloning objective. In this work, we present\\nDynaMo, a new in-domain, self-supervised method for learning visual\\nrepresentations. Given a set of expert demonstrations, we jointly learn a\\nlatent inverse dynamics model and a forward dynamics model over a sequence of\\nimage embeddings, predicting the next frame in latent space, without\\naugmentations, contrastive sampling, or access to ground truth actions.\\nImportantly, DynaMo does not require any out-of-domain data such as Internet\\ndatasets or cross-embodied datasets. On a suite of six simulated and real\\nenvironments, we show that representations learned with DynaMo significantly\\nimprove downstream imitation learning performance over prior self-supervised\\nlearning objectives, and pretrained representations. Gains from using DynaMo\\nhold across policy classes such as Behavior Transformer, Diffusion Policy, MLP,\\nand nearest neighbors. Finally, we ablate over key components of DynaMo and\\nmeasure its impact on downstream policy performance. Robot videos are best\\nviewed at https://dynamo-ssl.github.io\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"response\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfe12dc-7ec3-41ee-aab6-79f8107c39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "bos_token = \"<bos>\"\n",
    "eos_token = \"<eos>\"\n",
    "user_start = \"<user>\"\n",
    "user_end = \"</user>\"\n",
    "assistant_start = \"<assistant>\"\n",
    "assistant_end = \"</assistant>\"\n",
    "pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5a5219-912c-4a07-b8ab-549275414f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format examples\n",
    "def format_example(instruction, response):\n",
    "    return f\"{bos_token}\\n{user_start}\\n{instruction}\\n{user_end}\\n{assistant_start}\\n{response}\\n{assistant_end}\\n{eos_token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98bc350-4b67-4244-87c0-c67813d0ea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data.apply(lambda row: format_example(row['instruction'], row['response']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb5fe9f-aa1b-4aa4-99e1-e1608a696cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the BitsAndBytesConfig for 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load model in 8-bit precision\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fad6b92-cce3-4b0a-a83e-2f17347a5074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a327fa4aa45e490c7b6995c946601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the model and configure it\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Setup the BitsAndBytesConfig for 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Load model in 8-bit precision\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key, \n",
    ")\n",
    "\n",
    "special_tokens = {\n",
    "    'bos_token': bos_token,\n",
    "    'eos_token': eos_token,\n",
    "    'pad_token': pad_token,\n",
    "    'additional_special_tokens': [user_start, user_end, assistant_start, assistant_end]\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = pad_token  # Set pad_token to the unique pad_token\n",
    "\n",
    "# Set pad_token as eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                             token=api_key, \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             cache_dir=\"/project/models\",\n",
    "                                             device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c6a048-9285-4032-aaa7-610013e28b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128263, 4096)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56440cc5-ef96-4584-a9a4-c949cc02fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f016f013-6e13-41f6-903d-3a8f2e6f1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "tokenized_data = tokenizer(\n",
    "    data['text'].tolist(),\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb24df5e-24b7-4037-b322-421b7e421ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "class PapersDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data['input_ids']\n",
    "        self.attention_mask = tokenized_data['attention_mask']\n",
    "        self.labels = tokenized_data['input_ids'].clone()\n",
    "\n",
    "        assistant_start_id = tokenizer.convert_tokens_to_ids(assistant_start)\n",
    "        assistant_end_id = tokenizer.convert_tokens_to_ids(assistant_end)\n",
    "\n",
    "        for i in range(len(self.labels)):\n",
    "            input_ids = self.input_ids[i]\n",
    "            labels = self.labels[i]\n",
    "\n",
    "            assistant_start_positions = (input_ids == assistant_start_id).nonzero(as_tuple=True)[0]\n",
    "            assistant_end_positions = (input_ids == assistant_end_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            if len(assistant_start_positions) > 0 and len(assistant_end_positions) > 0:\n",
    "                assistant_start_pos = assistant_start_positions[0]\n",
    "                assistant_end_pos = assistant_end_positions[0]\n",
    "\n",
    "                labels[:assistant_start_pos + 1] = -100\n",
    "                labels[assistant_end_pos:] = -100\n",
    "            else:\n",
    "                labels[:] = -100\n",
    "\n",
    "            self.labels[i] = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e264f23-6728-431f-b2dd-a007d5bd1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PapersDataset(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797798e1-ef05-4fbc-8278-f221a43bb064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PapersDataset at 0x7f0d946c3cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fcccf5-7bba-4bf2-9010-6cef157d9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    max_steps = 100,\n",
    "    eval_strategy=\"no\",\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc2d55f0-3fe9-4fc7-a2ad-53fab26289f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments setting for 1 RTX 4090\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"/project/models/NV-arxiv-llama3.1\",             # Where to save results\n",
    "    num_train_epochs=3,                 # Number of epochs\n",
    "    per_device_train_batch_size=2,      # Start with 2, adjust based on memory\n",
    "    gradient_accumulation_steps=5,      # Accumulate gradients to simulate larger batch size\n",
    "    fp16=True,                         # Use FP16 for memory efficiency on RTX 4090\n",
    "    gradient_checkpointing=True,        # Enable gradient checkpointing to save memory\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2e-5,                 # Adjust learning rate for fine-tuning\n",
    "    max_grad_norm=0.3,                  # Gradient clipping\n",
    "    weight_decay=0.001,                 # Regularization\n",
    "    optim=\"adamw_torch\",                      # Use standard AdamW optimizer\n",
    "    max_steps=1500,                      # Train for 500 steps\n",
    "    warmup_ratio=0.03,                  # Warmup learning rate\n",
    "    group_by_length=True,               # Group sequences of similar lengths to save memory\n",
    "    save_steps=100,                     # Save model checkpoint every 100 steps\n",
    "    logging_steps=5,                    # Log training progress every 5 steps\n",
    "    report_to=\"none\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14d983ad-19c5-4049-8bf4-dca600179cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60b76cbd-434d-4374-a4e7-12e29d6caa92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:19:36, Epoch 187/188]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.837400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.848500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.846500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.792100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.692700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.611100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.377600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.133200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.932200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.818200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.955900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.789300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.670500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0.701200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>0.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>0.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>0.559500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.524500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>0.513600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>0.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>0.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>0.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>0.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>0.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.281500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>0.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>0.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.175800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>0.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>0.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.189900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>0.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.151800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>0.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.112000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>0.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>0.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.042000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>0.032500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>0.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>0.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>0.016100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>0.009800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>0.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.004900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee674b-6898c43c236916eb7d4bf20b;85bfb05c-1075-4dbc-bd30-3c869e2a558d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee688a-3cbce1861198c805050c90bf;59428482-28d0-4d8b-8ddc-8fdf2062c0e7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee69c8-31899de64f30fe595d034d9c;2ed05432-4746-4f84-a2c7-3952bb63d867)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee6b07-2e7e35aa00d3513928fb0282;5a5e0063-f2ab-4e4d-9f9d-4c67db703760)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee6c45-623709734ab6ecce4c0629e2;437250c8-837e-4f30-8c06-dd4d29191999)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee6d84-423491eb26ab14f72292df5f;3ac3a841-9d8b-4eb4-a459-d9a8a6fb56a1)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee6ec3-2c442abb575047d06d8e58b2;d15a18f2-896c-4178-b9eb-39a340624c75)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee7001-5ba1628e7f541f266df40f23;854b448a-6666-4947-910c-75342b7408b3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee7140-410bbebd79b766d4667ab2d7;a209286c-8bf9-4e23-bd56-cd817125d947)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee727f-5330789218e61d581727a983;16163e60-ad1f-47c4-9582-deb01aad3fc4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee73bd-11ad5705219bad8603e69cac;39582856-6564-4e3a-bba3-971c52eb46ae)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee74fc-12261b1f6b9a31e16aeb42be;11c61930-30de-4ed0-a395-b3db21d8a976)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee763b-085c11e6750e285a5fcdad46;4516fe05-10a9-4356-afe4-12c99f7835f7)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee7779-50ef3aa97b69b0bf4986213b;6e0737db-29fc-4ebc-8d31-7e8e867a6fd3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ee78b8-58c1d72a50699b4c673900f8;c932955f-94f2-42f7-9b1c-9f7a8975fed9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.4531424058731645, metrics={'train_runtime': 4780.0427, 'train_samples_per_second': 3.138, 'train_steps_per_second': 0.314, 'total_flos': 3.4645638905856e+17, 'train_loss': 0.4531424058731645, 'epoch': 187.5})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec585e2-bf6b-42d2-b1a6-180370c696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define your special tokens\n",
    "bos_token = \"<bos>\"\n",
    "eos_token = \"<eos>\"\n",
    "pad_token = \"<pad>\"\n",
    "user_start = \"<user>\"\n",
    "user_end = \"</user>\"\n",
    "assistant_start = \"<assistant>\"\n",
    "assistant_end = \"</assistant>\"\n",
    "\n",
    "special_tokens = {\n",
    "    'bos_token': bos_token,\n",
    "    'eos_token': eos_token,\n",
    "    'pad_token': pad_token,\n",
    "    'additional_special_tokens': [user_start, user_end, assistant_start, assistant_end]\n",
    "}\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key\n",
    ")\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baa03ed-93d6-43ea-8436-6e5a65d680a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c07a6a323d42f1b7f7016b74faf286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128263, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/project/models\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Update model's embeddings to accommodate new tokens\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47b9cd9e-677c-46d8-9420-19ceef185eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LoRA adapter weights\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"/project/models/arxiv_model\",\n",
    "    device_map=\"auto\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b937ac37-c239-4ff2-b21b-e181e8d01373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the format_example function\n",
    "def format_example(instruction, response=\"\"):\n",
    "    return f\"{bos_token}\\n{user_start}\\n{instruction}\\n{user_end}\\n{assistant_start}\\n{response}\"\n",
    "\n",
    "# Prepare the input\n",
    "instruction = \"I am looking for a paper discussing To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning\"\n",
    "input_text = format_example(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e19b5bb-39d4-4a27-8029-90a139e3db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dec218e4-0711-4180-b410-984eca222d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><bos>\n",
      "<user>\n",
      "I am looking for a paper discussing To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning\n",
      "</user>\n",
      "<assistant>\n",
      "I am looking for a paper discussing 'To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning'\n",
      " YYS\n",
      "One paper that discusses this topic is 'To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning'. Recent large language models have demonstrated impressive mathematical\n",
      "and logical reasoning capabilities. However, the complexity of natural math\n",
      "and logic problems often prevents models from finding an exact solution,\n",
      "leading them to opt for heuristic search strategies over an exhaustive branch\n",
      "exploration. In this work, we investigate the impact of the chain-of-thought\n",
      "(cot) curriculum on large language model's math and logic performance.\n",
      "Surprisingly, we find that the introduction of cot enhances model performance\n",
      "across a wide range of math and logic challenges, despite model-sized details\n",
      "available online that challenge standard evaluation. Furthermore, beyond\n",
      "equivalently performing to an exactly-solved model, we find that cot-trained\n",
      "models hold an significant advantage when competing against a solving process:\n",
      "On average, in tasks where a traditional model generates an answer, a trained\n",
      "cot-model scores 3% to 5% better than a process-following model. Finally, our\n",
      "results suggest that, while all open-source data\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(eos_token),\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    )\n",
    "\n",
    "# Decode and extract the assistant's response\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0042cbc4-faf3-403f-8da3-7c02c2dd4ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: base_model.model.model.embed_tokens.weight, Requires Grad: False, Shape: torch.Size([128263, 4096])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.0.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.0.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.0.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.0.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.0.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.1.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.1.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.1.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.1.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.1.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.2.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.2.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.2.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.2.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.2.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.3.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.3.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.3.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.3.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.3.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.4.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.4.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.4.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.4.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.4.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.5.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.5.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.5.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.5.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.5.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.6.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.6.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.6.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.6.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.6.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.7.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.7.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.7.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.7.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.7.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.8.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.8.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.8.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.8.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.8.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.9.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.9.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.9.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.9.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.9.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.10.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.10.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.10.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.10.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.10.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.11.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.11.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.11.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.11.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.11.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.12.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.12.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.12.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.12.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.12.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.13.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.13.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.13.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.13.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.13.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.14.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.14.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.14.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.14.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.14.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.15.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.15.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.15.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.15.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.15.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.16.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.16.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.16.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.16.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.16.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.17.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.17.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.17.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.17.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.17.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.18.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.18.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.18.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.18.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.18.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.19.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.19.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.19.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.19.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.19.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.20.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.20.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.20.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.20.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.20.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.21.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.21.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.21.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.21.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.21.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.22.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.22.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.22.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.22.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.22.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.23.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.23.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.23.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.23.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.23.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.24.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.24.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.24.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.24.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.24.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.25.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.25.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.25.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.25.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.25.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.26.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.26.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.26.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.26.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.26.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.27.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.27.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.27.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.27.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.27.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.28.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.28.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.28.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.28.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.28.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.29.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.29.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.29.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.29.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.29.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.30.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.30.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.30.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.30.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.30.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([2097152, 1])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([1024, 8])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight, Requires Grad: False, Shape: torch.Size([8388608, 1])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight, Requires Grad: False, Shape: torch.Size([8, 4096])\n",
      "Parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight, Requires Grad: False, Shape: torch.Size([4096, 8])\n",
      "Parameter: base_model.model.model.layers.31.mlp.gate_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.31.mlp.up_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.31.mlp.down_proj.weight, Requires Grad: False, Shape: torch.Size([29360128, 1])\n",
      "Parameter: base_model.model.model.layers.31.input_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.layers.31.post_attention_layernorm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.model.norm.weight, Requires Grad: False, Shape: torch.Size([4096])\n",
      "Parameter: base_model.model.lm_head.weight, Requires Grad: False, Shape: torch.Size([128263, 4096])\n"
     ]
    }
   ],
   "source": [
    "# List all parameters, including their requires_grad status\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Requires Grad: {param.requires_grad}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0d267d4-6416-4fa4-b8cb-6395b6624c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66ef1395-1d52f0d5647ae7d10bef27d7;241d2569-6f57-438f-9dac-30d9e15de56f)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Meta-Llama-3.1-8B-Instruct.\n",
      "  warnings.warn(\n",
      "/home/workbench/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Meta-Llama-3.1-8B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the LoRA adapter weights\n",
    "model.save_pretrained(\"/project/models/arxiv_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1df5c3a-4f2e-4aed-b706-59659608724c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00f1f4dde9a45a7ac2dfbf64164f5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inference\n",
    "# Load the base model and tokenizer with special tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key\n",
    ")\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "tokenizer.pad_token = pad_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/project/models\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Update model's embeddings\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"/project/models/arxiv_model\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e9f9953-200a-4ac7-9990-3482d37ee77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input\n",
    "instruction = \"I'm looking for papers discussing You Only Read Once(YORO).\"\n",
    "input_text = format_example(instruction, \"\")  # Empty assistant response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e89eb43-a3ad-4bad-833d-5d64c08689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    padding = True,\n",
    ").to(\"cuda\")\n",
    "\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b2401ba-1470-49b4-9f8a-078dad233507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><bos>\n",
      "<user>\n",
      "I'm looking for papers discussing You Only Read Once(YORO).\n",
      "</user>\n",
      "<assistant>\n",
      "One paper that discusses this topic is 'You Only Read Once (YORO): A New Intuitive\n",
      "Transformer Baseline for Long-Range Language Modeling'. The common practice in\n",
      "long-range language modeling is to multiply the length of the input by the\n",
      "number of tokens using a separator, which significantly hampers the\n",
      "performance of transformer-based models. To address this issue, we propose\n",
      "You Only Read Once (YORO), a simple transformation that fuses all tokens into\n",
      "a single input word by adding a learnable vector to the token embeddings.\n",
      "YORO does not require any changes to the underlying model architecture and can\n",
      "be applied to both encoder-decoders and encoders-only models. We evaluate\n",
      "YORO on a wide range of language modeling tasks, spanning both finite and\n",
      "infinite vocabulary corpora. Our results show that YORO consistently\n",
      "significantly improves the performance and efficiency of transformer-based\n",
      "models.\n",
      "Specifically, for finite vocabulary models with a separator, YORO closes the\n",
      "gap to much larger models. On infinite vocabulary tasks, YORO matches the\n",
      "performance of much larger GPT3.5 models, while using significantly fewer\n",
      "parameters. Finally, our ablations show that YORO outperforms other methods for\n",
      "addressing this problem, which either underperform or are only effective for\n",
      "very long inputs. Code is available at this https URL\n",
      "Code is available at this https URL.\n",
      "YORO is a simple, intuitive method for improving the performance of\n",
      "transformer-based language models. We hope that YORO will become a standard\n",
      "method for model developers, and will inspire further research into methods\n",
      "for addressing the challenges of long-range language modeling.\n",
      "You only read once(YORO) is best performed on language modeling. There, YORO\n",
      "adds no extra parameters and only 0.2% extra computation. In other tasks,\n",
      "YORO requires additional learnable parameters and increases computation by 4.8%.\n",
      "We thus evaluate YORO on language modeling. There, the most common practice is\n",
      "to multiply the length of the input by the number of tokens. For example, given\n",
      "a sequence of 10 short tokens, the common practice is to treat it as a single\n",
      "50-token input by adding 10 separators. Unfortunately, as the number of\n",
      "tokens increases, this approach hampers the performance\n"
     ]
    }
   ],
   "source": [
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    output = peft_model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=512,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(eos_token),\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(pad_token)\n",
    "    )\n",
    "\n",
    "# Decode and extract the assistant's response\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f84c0b2-1690-42ea-a585-1e65c9f2bc32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Token: <|begin_of_text|>, Label Token: <mask>\n",
      "Input Token: <bos>, Label Token: <mask>\n",
      "Input Token: ÄŠ, Label Token: <mask>\n",
      "Input Token: <user>, Label Token: <mask>\n",
      "Input Token: ÄŠ, Label Token: <mask>\n",
      "Input Token: I, Label Token: <mask>\n",
      "Input Token: 'm, Label Token: <mask>\n",
      "Input Token: Ä looking, Label Token: <mask>\n",
      "Input Token: Ä for, Label Token: <mask>\n",
      "Input Token: Ä papers, Label Token: <mask>\n",
      "Input Token: Ä discussing, Label Token: <mask>\n",
      "Input Token: Ä agents, Label Token: <mask>\n",
      "Input Token: Ä in, Label Token: <mask>\n",
      "Input Token: Ä software, Label Token: <mask>\n",
      "Input Token: Ä engineering, Label Token: <mask>\n",
      "Input Token: Ä survey, Label Token: <mask>\n",
      "Input Token: Ä landscape, Label Token: <mask>\n",
      "Input Token: Ä and, Label Token: <mask>\n",
      "Input Token: Ä vision, Label Token: <mask>\n",
      "Input Token: .ÄŠ, Label Token: <mask>\n",
      "Input Token: </user>, Label Token: <mask>\n",
      "Input Token: ÄŠ, Label Token: <mask>\n",
      "Input Token: <assistant>, Label Token: <mask>\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: One, Label Token: One\n",
      "Input Token: Ä paper, Label Token: Ä paper\n",
      "Input Token: Ä that, Label Token: Ä that\n",
      "Input Token: Ä discusses, Label Token: Ä discusses\n",
      "Input Token: Ä this, Label Token: Ä this\n",
      "Input Token: Ä topic, Label Token: Ä topic\n",
      "Input Token: Ä is, Label Token: Ä is\n",
      "Input Token: Ä ', Label Token: Ä '\n",
      "Input Token: Agents, Label Token: Agents\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä Software, Label Token: Ä Software\n",
      "Input Token: Ä Engineering, Label Token: Ä Engineering\n",
      "Input Token: :, Label Token: :\n",
      "Input Token: Ä Survey, Label Token: Ä Survey\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä Landscape, Label Token: Ä Landscape\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä and, Label Token: Ä and\n",
      "Input Token: Ä Vision, Label Token: Ä Vision\n",
      "Input Token: '., Label Token: '.\n",
      "Input Token: Ä In, Label Token: Ä In\n",
      "Input Token: Ä recent, Label Token: Ä recent\n",
      "Input Token: Ä years, Label Token: Ä years\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä Large, Label Token: Ä Large\n",
      "Input Token: Ä Language, Label Token: Ä Language\n",
      "Input Token: Ä Models, Label Token: Ä Models\n",
      "Input Token: Ä (, Label Token: Ä (\n",
      "Input Token: LL, Label Token: LL\n",
      "Input Token: Ms, Label Token: Ms\n",
      "Input Token: ), Label Token: )\n",
      "Input Token: Ä have, Label Token: Ä have\n",
      "Input Token: Ä achieved, Label Token: Ä achieved\n",
      "Input Token: Ä remarkable, Label Token: Ä remarkable\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: success, Label Token: success\n",
      "Input Token: Ä and, Label Token: Ä and\n",
      "Input Token: Ä have, Label Token: Ä have\n",
      "Input Token: Ä been, Label Token: Ä been\n",
      "Input Token: Ä widely, Label Token: Ä widely\n",
      "Input Token: Ä used, Label Token: Ä used\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä various, Label Token: Ä various\n",
      "Input Token: Ä downstream, Label Token: Ä downstream\n",
      "Input Token: Ä tasks, Label Token: Ä tasks\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä especially, Label Token: Ä especially\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: the, Label Token: the\n",
      "Input Token: Ä tasks, Label Token: Ä tasks\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä software, Label Token: Ä software\n",
      "Input Token: Ä engineering, Label Token: Ä engineering\n",
      "Input Token: Ä (, Label Token: Ä (\n",
      "Input Token: SE, Label Token: SE\n",
      "Input Token: ), Label Token: )\n",
      "Input Token: Ä field, Label Token: Ä field\n",
      "Input Token: ., Label Token: .\n",
      "Input Token: Ä We, Label Token: Ä We\n",
      "Input Token: Ä find, Label Token: Ä find\n",
      "Input Token: Ä that, Label Token: Ä that\n",
      "Input Token: Ä many, Label Token: Ä many\n",
      "Input Token: Ä studies, Label Token: Ä studies\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: comb, Label Token: comb\n",
      "Input Token: ining, Label Token: ining\n",
      "Input Token: Ä L, Label Token: Ä L\n",
      "Input Token: LM, Label Token: LM\n",
      "Input Token: s, Label Token: s\n",
      "Input Token: Ä with, Label Token: Ä with\n",
      "Input Token: Ä SE, Label Token: Ä SE\n",
      "Input Token: Ä have, Label Token: Ä have\n",
      "Input Token: Ä employed, Label Token: Ä employed\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä concept, Label Token: Ä concept\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä agents, Label Token: Ä agents\n",
      "Input Token: Ä either, Label Token: Ä either\n",
      "Input Token: Ä explicitly, Label Token: Ä explicitly\n",
      "Input Token: Ä or, Label Token: Ä or\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: implicitly, Label Token: implicitly\n",
      "Input Token: ., Label Token: .\n",
      "Input Token: Ä However, Label Token: Ä However\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä there, Label Token: Ä there\n",
      "Input Token: Ä is, Label Token: Ä is\n",
      "Input Token: Ä a, Label Token: Ä a\n",
      "Input Token: Ä lack, Label Token: Ä lack\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä an, Label Token: Ä an\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: -depth, Label Token: -depth\n",
      "Input Token: Ä survey, Label Token: Ä survey\n",
      "Input Token: Ä to, Label Token: Ä to\n",
      "Input Token: Ä sort, Label Token: Ä sort\n",
      "Input Token: Ä out, Label Token: Ä out\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: development, Label Token: development\n",
      "Input Token: Ä context, Label Token: Ä context\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä existing, Label Token: Ä existing\n",
      "Input Token: Ä works, Label Token: Ä works\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä analyze, Label Token: Ä analyze\n",
      "Input Token: Ä how, Label Token: Ä how\n",
      "Input Token: Ä existing, Label Token: Ä existing\n",
      "Input Token: Ä works, Label Token: Ä works\n",
      "Input Token: Ä combine, Label Token: Ä combine\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: LL, Label Token: LL\n",
      "Input Token: M, Label Token: M\n",
      "Input Token: -based, Label Token: -based\n",
      "Input Token: Ä agent, Label Token: Ä agent\n",
      "Input Token: Ä technologies, Label Token: Ä technologies\n",
      "Input Token: Ä to, Label Token: Ä to\n",
      "Input Token: Ä optimize, Label Token: Ä optimize\n",
      "Input Token: Ä various, Label Token: Ä various\n",
      "Input Token: Ä tasks, Label Token: Ä tasks\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä and, Label Token: Ä and\n",
      "Input Token: Ä clarify, Label Token: Ä clarify\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: framework, Label Token: framework\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä L, Label Token: Ä L\n",
      "Input Token: LM, Label Token: LM\n",
      "Input Token: -based, Label Token: -based\n",
      "Input Token: Ä agents, Label Token: Ä agents\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä SE, Label Token: Ä SE\n",
      "Input Token: ., Label Token: .\n",
      "Input Token: Ä In, Label Token: Ä In\n",
      "Input Token: Ä this, Label Token: Ä this\n",
      "Input Token: Ä paper, Label Token: Ä paper\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä we, Label Token: Ä we\n",
      "Input Token: Ä conduct, Label Token: Ä conduct\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä first, Label Token: Ä first\n",
      "Input Token: Ä survey, Label Token: Ä survey\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: of, Label Token: of\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä studies, Label Token: Ä studies\n",
      "Input Token: Ä on, Label Token: Ä on\n",
      "Input Token: Ä combining, Label Token: Ä combining\n",
      "Input Token: Ä L, Label Token: Ä L\n",
      "Input Token: LM, Label Token: LM\n",
      "Input Token: -based, Label Token: -based\n",
      "Input Token: Ä agents, Label Token: Ä agents\n",
      "Input Token: Ä with, Label Token: Ä with\n",
      "Input Token: Ä SE, Label Token: Ä SE\n",
      "Input Token: Ä and, Label Token: Ä and\n",
      "Input Token: Ä present, Label Token: Ä present\n",
      "Input Token: Ä a, Label Token: Ä a\n",
      "Input Token: Ä framework, Label Token: Ä framework\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: LL, Label Token: LL\n",
      "Input Token: M, Label Token: M\n",
      "Input Token: -based, Label Token: -based\n",
      "Input Token: Ä agents, Label Token: Ä agents\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä SE, Label Token: Ä SE\n",
      "Input Token: Ä which, Label Token: Ä which\n",
      "Input Token: Ä includes, Label Token: Ä includes\n",
      "Input Token: Ä three, Label Token: Ä three\n",
      "Input Token: Ä key, Label Token: Ä key\n",
      "Input Token: Ä modules, Label Token: Ä modules\n",
      "Input Token: :, Label Token: :\n",
      "Input Token: Ä perception, Label Token: Ä perception\n",
      "Input Token: ,, Label Token: ,\n",
      "Input Token: Ä memory, Label Token: Ä memory\n",
      "Input Token: ,ÄŠ, Label Token: ,ÄŠ\n",
      "Input Token: and, Label Token: and\n",
      "Input Token: Ä action, Label Token: Ä action\n",
      "Input Token: ., Label Token: .\n",
      "Input Token: Ä We, Label Token: Ä We\n",
      "Input Token: Ä also, Label Token: Ä also\n",
      "Input Token: Ä summarize, Label Token: Ä summarize\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä current, Label Token: Ä current\n",
      "Input Token: Ä challenges, Label Token: Ä challenges\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä combining, Label Token: Ä combining\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä two, Label Token: Ä two\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: fields, Label Token: fields\n",
      "Input Token: Ä and, Label Token: Ä and\n",
      "Input Token: Ä propose, Label Token: Ä propose\n",
      "Input Token: Ä future, Label Token: Ä future\n",
      "Input Token: Ä opportunities, Label Token: Ä opportunities\n",
      "Input Token: Ä in, Label Token: Ä in\n",
      "Input Token: Ä response, Label Token: Ä response\n",
      "Input Token: Ä to, Label Token: Ä to\n",
      "Input Token: Ä existing, Label Token: Ä existing\n",
      "Input Token: Ä challenges, Label Token: Ä challenges\n",
      "Input Token: ., Label Token: .\n",
      "Input Token: Ä We, Label Token: Ä We\n",
      "Input Token: ÄŠ, Label Token: ÄŠ\n",
      "Input Token: maint, Label Token: maint\n",
      "Input Token: ain, Label Token: ain\n",
      "Input Token: Ä a, Label Token: Ä a\n",
      "Input Token: Ä GitHub, Label Token: Ä GitHub\n",
      "Input Token: Ä repository, Label Token: Ä repository\n",
      "Input Token: Ä of, Label Token: Ä of\n",
      "Input Token: Ä the, Label Token: Ä the\n",
      "Input Token: Ä related, Label Token: Ä related\n",
      "Input Token: Ä papers, Label Token: Ä papers\n",
      "Input Token: Ä at, Label Token: Ä at\n",
      "Input Token: :ÄŠ, Label Token: :ÄŠ\n",
      "Input Token: https, Label Token: https\n",
      "Input Token: ://, Label Token: ://\n",
      "Input Token: github, Label Token: github\n",
      "Input Token: .com, Label Token: .com\n",
      "Input Token: /, Label Token: /\n",
      "Input Token: Deep, Label Token: Deep\n",
      "Input Token: Software, Label Token: Software\n",
      "Input Token: Analytics, Label Token: Analytics\n",
      "Input Token: /A, Label Token: /A\n",
      "Input Token: w, Label Token: w\n",
      "Input Token: esome, Label Token: esome\n",
      "Input Token: -Agent, Label Token: -Agent\n",
      "Input Token: 4, Label Token: 4\n",
      "Input Token: SE, Label Token: SE\n",
      "Input Token: .ÄŠ, Label Token: .ÄŠ\n",
      "Input Token: </assistant>, Label Token: <mask>\n",
      "Input Token: ÄŠ, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n",
      "Input Token: <eos>, Label Token: <mask>\n"
     ]
    }
   ],
   "source": [
    "# Get an example from your dataset\n",
    "example = dataset[0]\n",
    "input_ids = example['input_ids']\n",
    "labels = example['labels']\n",
    "\n",
    "# Convert token IDs back to tokens for readability\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "label_tokens = [\n",
    "    tokenizer.convert_ids_to_tokens([id])[0] if id != -100 else '<mask>'\n",
    "    for id in labels\n",
    "]\n",
    "\n",
    "# Print tokens side by side\n",
    "for input_token, label_token in zip(input_tokens, label_tokens):\n",
    "    print(f\"Input Token: {input_token}, Label Token: {label_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294abc2f-b5bd-4622-b5e1-2cf3f0fcd3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Shape: torch.Size([1024, 8])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight, Shape: torch.Size([8, 4096])\n",
      "Check\n",
      "Trainable parameter: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight, Shape: torch.Size([4096, 8])\n"
     ]
    }
   ],
   "source": [
    "# Check trainable parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(\"Check\")\n",
    "        print(f\"Trainable parameter: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe8003a-9932-48c8-840f-c92bd565419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9656686904fa4f98a0bbdb7cf301cebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is important correct one\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=api_key,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/project/models\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Update model's embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # Ensure these modules exist in your model\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4126bb19-1b98-4207-a8ac-861b482c1578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "base_model\n",
      "base_model.model\n",
      "base_model.model.model\n",
      "base_model.model.model.embed_tokens\n",
      "base_model.model.model.layers\n",
      "base_model.model.model.layers.0\n",
      "base_model.model.model.layers.0.self_attn\n",
      "base_model.model.model.layers.0.self_attn.q_proj\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.k_proj\n",
      "base_model.model.model.layers.0.self_attn.v_proj\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.0.self_attn.o_proj\n",
      "base_model.model.model.layers.0.self_attn.rotary_emb\n",
      "base_model.model.model.layers.0.mlp\n",
      "base_model.model.model.layers.0.mlp.gate_proj\n",
      "base_model.model.model.layers.0.mlp.up_proj\n",
      "base_model.model.model.layers.0.mlp.down_proj\n",
      "base_model.model.model.layers.0.mlp.act_fn\n",
      "base_model.model.model.layers.0.input_layernorm\n",
      "base_model.model.model.layers.0.post_attention_layernorm\n",
      "base_model.model.model.layers.1\n",
      "base_model.model.model.layers.1.self_attn\n",
      "base_model.model.model.layers.1.self_attn.q_proj\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.k_proj\n",
      "base_model.model.model.layers.1.self_attn.v_proj\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.1.self_attn.o_proj\n",
      "base_model.model.model.layers.1.self_attn.rotary_emb\n",
      "base_model.model.model.layers.1.mlp\n",
      "base_model.model.model.layers.1.mlp.gate_proj\n",
      "base_model.model.model.layers.1.mlp.up_proj\n",
      "base_model.model.model.layers.1.mlp.down_proj\n",
      "base_model.model.model.layers.1.mlp.act_fn\n",
      "base_model.model.model.layers.1.input_layernorm\n",
      "base_model.model.model.layers.1.post_attention_layernorm\n",
      "base_model.model.model.layers.2\n",
      "base_model.model.model.layers.2.self_attn\n",
      "base_model.model.model.layers.2.self_attn.q_proj\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.k_proj\n",
      "base_model.model.model.layers.2.self_attn.v_proj\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.2.self_attn.o_proj\n",
      "base_model.model.model.layers.2.self_attn.rotary_emb\n",
      "base_model.model.model.layers.2.mlp\n",
      "base_model.model.model.layers.2.mlp.gate_proj\n",
      "base_model.model.model.layers.2.mlp.up_proj\n",
      "base_model.model.model.layers.2.mlp.down_proj\n",
      "base_model.model.model.layers.2.mlp.act_fn\n",
      "base_model.model.model.layers.2.input_layernorm\n",
      "base_model.model.model.layers.2.post_attention_layernorm\n",
      "base_model.model.model.layers.3\n",
      "base_model.model.model.layers.3.self_attn\n",
      "base_model.model.model.layers.3.self_attn.q_proj\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.k_proj\n",
      "base_model.model.model.layers.3.self_attn.v_proj\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.3.self_attn.o_proj\n",
      "base_model.model.model.layers.3.self_attn.rotary_emb\n",
      "base_model.model.model.layers.3.mlp\n",
      "base_model.model.model.layers.3.mlp.gate_proj\n",
      "base_model.model.model.layers.3.mlp.up_proj\n",
      "base_model.model.model.layers.3.mlp.down_proj\n",
      "base_model.model.model.layers.3.mlp.act_fn\n",
      "base_model.model.model.layers.3.input_layernorm\n",
      "base_model.model.model.layers.3.post_attention_layernorm\n",
      "base_model.model.model.layers.4\n",
      "base_model.model.model.layers.4.self_attn\n",
      "base_model.model.model.layers.4.self_attn.q_proj\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.k_proj\n",
      "base_model.model.model.layers.4.self_attn.v_proj\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.4.self_attn.o_proj\n",
      "base_model.model.model.layers.4.self_attn.rotary_emb\n",
      "base_model.model.model.layers.4.mlp\n",
      "base_model.model.model.layers.4.mlp.gate_proj\n",
      "base_model.model.model.layers.4.mlp.up_proj\n",
      "base_model.model.model.layers.4.mlp.down_proj\n",
      "base_model.model.model.layers.4.mlp.act_fn\n",
      "base_model.model.model.layers.4.input_layernorm\n",
      "base_model.model.model.layers.4.post_attention_layernorm\n",
      "base_model.model.model.layers.5\n",
      "base_model.model.model.layers.5.self_attn\n",
      "base_model.model.model.layers.5.self_attn.q_proj\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.k_proj\n",
      "base_model.model.model.layers.5.self_attn.v_proj\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.5.self_attn.o_proj\n",
      "base_model.model.model.layers.5.self_attn.rotary_emb\n",
      "base_model.model.model.layers.5.mlp\n",
      "base_model.model.model.layers.5.mlp.gate_proj\n",
      "base_model.model.model.layers.5.mlp.up_proj\n",
      "base_model.model.model.layers.5.mlp.down_proj\n",
      "base_model.model.model.layers.5.mlp.act_fn\n",
      "base_model.model.model.layers.5.input_layernorm\n",
      "base_model.model.model.layers.5.post_attention_layernorm\n",
      "base_model.model.model.layers.6\n",
      "base_model.model.model.layers.6.self_attn\n",
      "base_model.model.model.layers.6.self_attn.q_proj\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.k_proj\n",
      "base_model.model.model.layers.6.self_attn.v_proj\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.6.self_attn.o_proj\n",
      "base_model.model.model.layers.6.self_attn.rotary_emb\n",
      "base_model.model.model.layers.6.mlp\n",
      "base_model.model.model.layers.6.mlp.gate_proj\n",
      "base_model.model.model.layers.6.mlp.up_proj\n",
      "base_model.model.model.layers.6.mlp.down_proj\n",
      "base_model.model.model.layers.6.mlp.act_fn\n",
      "base_model.model.model.layers.6.input_layernorm\n",
      "base_model.model.model.layers.6.post_attention_layernorm\n",
      "base_model.model.model.layers.7\n",
      "base_model.model.model.layers.7.self_attn\n",
      "base_model.model.model.layers.7.self_attn.q_proj\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.k_proj\n",
      "base_model.model.model.layers.7.self_attn.v_proj\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.7.self_attn.o_proj\n",
      "base_model.model.model.layers.7.self_attn.rotary_emb\n",
      "base_model.model.model.layers.7.mlp\n",
      "base_model.model.model.layers.7.mlp.gate_proj\n",
      "base_model.model.model.layers.7.mlp.up_proj\n",
      "base_model.model.model.layers.7.mlp.down_proj\n",
      "base_model.model.model.layers.7.mlp.act_fn\n",
      "base_model.model.model.layers.7.input_layernorm\n",
      "base_model.model.model.layers.7.post_attention_layernorm\n",
      "base_model.model.model.layers.8\n",
      "base_model.model.model.layers.8.self_attn\n",
      "base_model.model.model.layers.8.self_attn.q_proj\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.k_proj\n",
      "base_model.model.model.layers.8.self_attn.v_proj\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.8.self_attn.o_proj\n",
      "base_model.model.model.layers.8.self_attn.rotary_emb\n",
      "base_model.model.model.layers.8.mlp\n",
      "base_model.model.model.layers.8.mlp.gate_proj\n",
      "base_model.model.model.layers.8.mlp.up_proj\n",
      "base_model.model.model.layers.8.mlp.down_proj\n",
      "base_model.model.model.layers.8.mlp.act_fn\n",
      "base_model.model.model.layers.8.input_layernorm\n",
      "base_model.model.model.layers.8.post_attention_layernorm\n",
      "base_model.model.model.layers.9\n",
      "base_model.model.model.layers.9.self_attn\n",
      "base_model.model.model.layers.9.self_attn.q_proj\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.k_proj\n",
      "base_model.model.model.layers.9.self_attn.v_proj\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.9.self_attn.o_proj\n",
      "base_model.model.model.layers.9.self_attn.rotary_emb\n",
      "base_model.model.model.layers.9.mlp\n",
      "base_model.model.model.layers.9.mlp.gate_proj\n",
      "base_model.model.model.layers.9.mlp.up_proj\n",
      "base_model.model.model.layers.9.mlp.down_proj\n",
      "base_model.model.model.layers.9.mlp.act_fn\n",
      "base_model.model.model.layers.9.input_layernorm\n",
      "base_model.model.model.layers.9.post_attention_layernorm\n",
      "base_model.model.model.layers.10\n",
      "base_model.model.model.layers.10.self_attn\n",
      "base_model.model.model.layers.10.self_attn.q_proj\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.k_proj\n",
      "base_model.model.model.layers.10.self_attn.v_proj\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.10.self_attn.o_proj\n",
      "base_model.model.model.layers.10.self_attn.rotary_emb\n",
      "base_model.model.model.layers.10.mlp\n",
      "base_model.model.model.layers.10.mlp.gate_proj\n",
      "base_model.model.model.layers.10.mlp.up_proj\n",
      "base_model.model.model.layers.10.mlp.down_proj\n",
      "base_model.model.model.layers.10.mlp.act_fn\n",
      "base_model.model.model.layers.10.input_layernorm\n",
      "base_model.model.model.layers.10.post_attention_layernorm\n",
      "base_model.model.model.layers.11\n",
      "base_model.model.model.layers.11.self_attn\n",
      "base_model.model.model.layers.11.self_attn.q_proj\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.k_proj\n",
      "base_model.model.model.layers.11.self_attn.v_proj\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.11.self_attn.o_proj\n",
      "base_model.model.model.layers.11.self_attn.rotary_emb\n",
      "base_model.model.model.layers.11.mlp\n",
      "base_model.model.model.layers.11.mlp.gate_proj\n",
      "base_model.model.model.layers.11.mlp.up_proj\n",
      "base_model.model.model.layers.11.mlp.down_proj\n",
      "base_model.model.model.layers.11.mlp.act_fn\n",
      "base_model.model.model.layers.11.input_layernorm\n",
      "base_model.model.model.layers.11.post_attention_layernorm\n",
      "base_model.model.model.layers.12\n",
      "base_model.model.model.layers.12.self_attn\n",
      "base_model.model.model.layers.12.self_attn.q_proj\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.k_proj\n",
      "base_model.model.model.layers.12.self_attn.v_proj\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.12.self_attn.o_proj\n",
      "base_model.model.model.layers.12.self_attn.rotary_emb\n",
      "base_model.model.model.layers.12.mlp\n",
      "base_model.model.model.layers.12.mlp.gate_proj\n",
      "base_model.model.model.layers.12.mlp.up_proj\n",
      "base_model.model.model.layers.12.mlp.down_proj\n",
      "base_model.model.model.layers.12.mlp.act_fn\n",
      "base_model.model.model.layers.12.input_layernorm\n",
      "base_model.model.model.layers.12.post_attention_layernorm\n",
      "base_model.model.model.layers.13\n",
      "base_model.model.model.layers.13.self_attn\n",
      "base_model.model.model.layers.13.self_attn.q_proj\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.k_proj\n",
      "base_model.model.model.layers.13.self_attn.v_proj\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.13.self_attn.o_proj\n",
      "base_model.model.model.layers.13.self_attn.rotary_emb\n",
      "base_model.model.model.layers.13.mlp\n",
      "base_model.model.model.layers.13.mlp.gate_proj\n",
      "base_model.model.model.layers.13.mlp.up_proj\n",
      "base_model.model.model.layers.13.mlp.down_proj\n",
      "base_model.model.model.layers.13.mlp.act_fn\n",
      "base_model.model.model.layers.13.input_layernorm\n",
      "base_model.model.model.layers.13.post_attention_layernorm\n",
      "base_model.model.model.layers.14\n",
      "base_model.model.model.layers.14.self_attn\n",
      "base_model.model.model.layers.14.self_attn.q_proj\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.k_proj\n",
      "base_model.model.model.layers.14.self_attn.v_proj\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.14.self_attn.o_proj\n",
      "base_model.model.model.layers.14.self_attn.rotary_emb\n",
      "base_model.model.model.layers.14.mlp\n",
      "base_model.model.model.layers.14.mlp.gate_proj\n",
      "base_model.model.model.layers.14.mlp.up_proj\n",
      "base_model.model.model.layers.14.mlp.down_proj\n",
      "base_model.model.model.layers.14.mlp.act_fn\n",
      "base_model.model.model.layers.14.input_layernorm\n",
      "base_model.model.model.layers.14.post_attention_layernorm\n",
      "base_model.model.model.layers.15\n",
      "base_model.model.model.layers.15.self_attn\n",
      "base_model.model.model.layers.15.self_attn.q_proj\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.k_proj\n",
      "base_model.model.model.layers.15.self_attn.v_proj\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.15.self_attn.o_proj\n",
      "base_model.model.model.layers.15.self_attn.rotary_emb\n",
      "base_model.model.model.layers.15.mlp\n",
      "base_model.model.model.layers.15.mlp.gate_proj\n",
      "base_model.model.model.layers.15.mlp.up_proj\n",
      "base_model.model.model.layers.15.mlp.down_proj\n",
      "base_model.model.model.layers.15.mlp.act_fn\n",
      "base_model.model.model.layers.15.input_layernorm\n",
      "base_model.model.model.layers.15.post_attention_layernorm\n",
      "base_model.model.model.layers.16\n",
      "base_model.model.model.layers.16.self_attn\n",
      "base_model.model.model.layers.16.self_attn.q_proj\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.k_proj\n",
      "base_model.model.model.layers.16.self_attn.v_proj\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.16.self_attn.o_proj\n",
      "base_model.model.model.layers.16.self_attn.rotary_emb\n",
      "base_model.model.model.layers.16.mlp\n",
      "base_model.model.model.layers.16.mlp.gate_proj\n",
      "base_model.model.model.layers.16.mlp.up_proj\n",
      "base_model.model.model.layers.16.mlp.down_proj\n",
      "base_model.model.model.layers.16.mlp.act_fn\n",
      "base_model.model.model.layers.16.input_layernorm\n",
      "base_model.model.model.layers.16.post_attention_layernorm\n",
      "base_model.model.model.layers.17\n",
      "base_model.model.model.layers.17.self_attn\n",
      "base_model.model.model.layers.17.self_attn.q_proj\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.k_proj\n",
      "base_model.model.model.layers.17.self_attn.v_proj\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.17.self_attn.o_proj\n",
      "base_model.model.model.layers.17.self_attn.rotary_emb\n",
      "base_model.model.model.layers.17.mlp\n",
      "base_model.model.model.layers.17.mlp.gate_proj\n",
      "base_model.model.model.layers.17.mlp.up_proj\n",
      "base_model.model.model.layers.17.mlp.down_proj\n",
      "base_model.model.model.layers.17.mlp.act_fn\n",
      "base_model.model.model.layers.17.input_layernorm\n",
      "base_model.model.model.layers.17.post_attention_layernorm\n",
      "base_model.model.model.layers.18\n",
      "base_model.model.model.layers.18.self_attn\n",
      "base_model.model.model.layers.18.self_attn.q_proj\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.k_proj\n",
      "base_model.model.model.layers.18.self_attn.v_proj\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.18.self_attn.o_proj\n",
      "base_model.model.model.layers.18.self_attn.rotary_emb\n",
      "base_model.model.model.layers.18.mlp\n",
      "base_model.model.model.layers.18.mlp.gate_proj\n",
      "base_model.model.model.layers.18.mlp.up_proj\n",
      "base_model.model.model.layers.18.mlp.down_proj\n",
      "base_model.model.model.layers.18.mlp.act_fn\n",
      "base_model.model.model.layers.18.input_layernorm\n",
      "base_model.model.model.layers.18.post_attention_layernorm\n",
      "base_model.model.model.layers.19\n",
      "base_model.model.model.layers.19.self_attn\n",
      "base_model.model.model.layers.19.self_attn.q_proj\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.k_proj\n",
      "base_model.model.model.layers.19.self_attn.v_proj\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.19.self_attn.o_proj\n",
      "base_model.model.model.layers.19.self_attn.rotary_emb\n",
      "base_model.model.model.layers.19.mlp\n",
      "base_model.model.model.layers.19.mlp.gate_proj\n",
      "base_model.model.model.layers.19.mlp.up_proj\n",
      "base_model.model.model.layers.19.mlp.down_proj\n",
      "base_model.model.model.layers.19.mlp.act_fn\n",
      "base_model.model.model.layers.19.input_layernorm\n",
      "base_model.model.model.layers.19.post_attention_layernorm\n",
      "base_model.model.model.layers.20\n",
      "base_model.model.model.layers.20.self_attn\n",
      "base_model.model.model.layers.20.self_attn.q_proj\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.k_proj\n",
      "base_model.model.model.layers.20.self_attn.v_proj\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.20.self_attn.o_proj\n",
      "base_model.model.model.layers.20.self_attn.rotary_emb\n",
      "base_model.model.model.layers.20.mlp\n",
      "base_model.model.model.layers.20.mlp.gate_proj\n",
      "base_model.model.model.layers.20.mlp.up_proj\n",
      "base_model.model.model.layers.20.mlp.down_proj\n",
      "base_model.model.model.layers.20.mlp.act_fn\n",
      "base_model.model.model.layers.20.input_layernorm\n",
      "base_model.model.model.layers.20.post_attention_layernorm\n",
      "base_model.model.model.layers.21\n",
      "base_model.model.model.layers.21.self_attn\n",
      "base_model.model.model.layers.21.self_attn.q_proj\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.k_proj\n",
      "base_model.model.model.layers.21.self_attn.v_proj\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.21.self_attn.o_proj\n",
      "base_model.model.model.layers.21.self_attn.rotary_emb\n",
      "base_model.model.model.layers.21.mlp\n",
      "base_model.model.model.layers.21.mlp.gate_proj\n",
      "base_model.model.model.layers.21.mlp.up_proj\n",
      "base_model.model.model.layers.21.mlp.down_proj\n",
      "base_model.model.model.layers.21.mlp.act_fn\n",
      "base_model.model.model.layers.21.input_layernorm\n",
      "base_model.model.model.layers.21.post_attention_layernorm\n",
      "base_model.model.model.layers.22\n",
      "base_model.model.model.layers.22.self_attn\n",
      "base_model.model.model.layers.22.self_attn.q_proj\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.22.self_attn.k_proj\n",
      "base_model.model.model.layers.22.self_attn.v_proj\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.22.self_attn.o_proj\n",
      "base_model.model.model.layers.22.self_attn.rotary_emb\n",
      "base_model.model.model.layers.22.mlp\n",
      "base_model.model.model.layers.22.mlp.gate_proj\n",
      "base_model.model.model.layers.22.mlp.up_proj\n",
      "base_model.model.model.layers.22.mlp.down_proj\n",
      "base_model.model.model.layers.22.mlp.act_fn\n",
      "base_model.model.model.layers.22.input_layernorm\n",
      "base_model.model.model.layers.22.post_attention_layernorm\n",
      "base_model.model.model.layers.23\n",
      "base_model.model.model.layers.23.self_attn\n",
      "base_model.model.model.layers.23.self_attn.q_proj\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.23.self_attn.k_proj\n",
      "base_model.model.model.layers.23.self_attn.v_proj\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.23.self_attn.o_proj\n",
      "base_model.model.model.layers.23.self_attn.rotary_emb\n",
      "base_model.model.model.layers.23.mlp\n",
      "base_model.model.model.layers.23.mlp.gate_proj\n",
      "base_model.model.model.layers.23.mlp.up_proj\n",
      "base_model.model.model.layers.23.mlp.down_proj\n",
      "base_model.model.model.layers.23.mlp.act_fn\n",
      "base_model.model.model.layers.23.input_layernorm\n",
      "base_model.model.model.layers.23.post_attention_layernorm\n",
      "base_model.model.model.layers.24\n",
      "base_model.model.model.layers.24.self_attn\n",
      "base_model.model.model.layers.24.self_attn.q_proj\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.24.self_attn.k_proj\n",
      "base_model.model.model.layers.24.self_attn.v_proj\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.24.self_attn.o_proj\n",
      "base_model.model.model.layers.24.self_attn.rotary_emb\n",
      "base_model.model.model.layers.24.mlp\n",
      "base_model.model.model.layers.24.mlp.gate_proj\n",
      "base_model.model.model.layers.24.mlp.up_proj\n",
      "base_model.model.model.layers.24.mlp.down_proj\n",
      "base_model.model.model.layers.24.mlp.act_fn\n",
      "base_model.model.model.layers.24.input_layernorm\n",
      "base_model.model.model.layers.24.post_attention_layernorm\n",
      "base_model.model.model.layers.25\n",
      "base_model.model.model.layers.25.self_attn\n",
      "base_model.model.model.layers.25.self_attn.q_proj\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.25.self_attn.k_proj\n",
      "base_model.model.model.layers.25.self_attn.v_proj\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.25.self_attn.o_proj\n",
      "base_model.model.model.layers.25.self_attn.rotary_emb\n",
      "base_model.model.model.layers.25.mlp\n",
      "base_model.model.model.layers.25.mlp.gate_proj\n",
      "base_model.model.model.layers.25.mlp.up_proj\n",
      "base_model.model.model.layers.25.mlp.down_proj\n",
      "base_model.model.model.layers.25.mlp.act_fn\n",
      "base_model.model.model.layers.25.input_layernorm\n",
      "base_model.model.model.layers.25.post_attention_layernorm\n",
      "base_model.model.model.layers.26\n",
      "base_model.model.model.layers.26.self_attn\n",
      "base_model.model.model.layers.26.self_attn.q_proj\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.26.self_attn.k_proj\n",
      "base_model.model.model.layers.26.self_attn.v_proj\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.26.self_attn.o_proj\n",
      "base_model.model.model.layers.26.self_attn.rotary_emb\n",
      "base_model.model.model.layers.26.mlp\n",
      "base_model.model.model.layers.26.mlp.gate_proj\n",
      "base_model.model.model.layers.26.mlp.up_proj\n",
      "base_model.model.model.layers.26.mlp.down_proj\n",
      "base_model.model.model.layers.26.mlp.act_fn\n",
      "base_model.model.model.layers.26.input_layernorm\n",
      "base_model.model.model.layers.26.post_attention_layernorm\n",
      "base_model.model.model.layers.27\n",
      "base_model.model.model.layers.27.self_attn\n",
      "base_model.model.model.layers.27.self_attn.q_proj\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.27.self_attn.k_proj\n",
      "base_model.model.model.layers.27.self_attn.v_proj\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.27.self_attn.o_proj\n",
      "base_model.model.model.layers.27.self_attn.rotary_emb\n",
      "base_model.model.model.layers.27.mlp\n",
      "base_model.model.model.layers.27.mlp.gate_proj\n",
      "base_model.model.model.layers.27.mlp.up_proj\n",
      "base_model.model.model.layers.27.mlp.down_proj\n",
      "base_model.model.model.layers.27.mlp.act_fn\n",
      "base_model.model.model.layers.27.input_layernorm\n",
      "base_model.model.model.layers.27.post_attention_layernorm\n",
      "base_model.model.model.layers.28\n",
      "base_model.model.model.layers.28.self_attn\n",
      "base_model.model.model.layers.28.self_attn.q_proj\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.28.self_attn.k_proj\n",
      "base_model.model.model.layers.28.self_attn.v_proj\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.28.self_attn.o_proj\n",
      "base_model.model.model.layers.28.self_attn.rotary_emb\n",
      "base_model.model.model.layers.28.mlp\n",
      "base_model.model.model.layers.28.mlp.gate_proj\n",
      "base_model.model.model.layers.28.mlp.up_proj\n",
      "base_model.model.model.layers.28.mlp.down_proj\n",
      "base_model.model.model.layers.28.mlp.act_fn\n",
      "base_model.model.model.layers.28.input_layernorm\n",
      "base_model.model.model.layers.28.post_attention_layernorm\n",
      "base_model.model.model.layers.29\n",
      "base_model.model.model.layers.29.self_attn\n",
      "base_model.model.model.layers.29.self_attn.q_proj\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.29.self_attn.k_proj\n",
      "base_model.model.model.layers.29.self_attn.v_proj\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.29.self_attn.o_proj\n",
      "base_model.model.model.layers.29.self_attn.rotary_emb\n",
      "base_model.model.model.layers.29.mlp\n",
      "base_model.model.model.layers.29.mlp.gate_proj\n",
      "base_model.model.model.layers.29.mlp.up_proj\n",
      "base_model.model.model.layers.29.mlp.down_proj\n",
      "base_model.model.model.layers.29.mlp.act_fn\n",
      "base_model.model.model.layers.29.input_layernorm\n",
      "base_model.model.model.layers.29.post_attention_layernorm\n",
      "base_model.model.model.layers.30\n",
      "base_model.model.model.layers.30.self_attn\n",
      "base_model.model.model.layers.30.self_attn.q_proj\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.30.self_attn.k_proj\n",
      "base_model.model.model.layers.30.self_attn.v_proj\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.30.self_attn.o_proj\n",
      "base_model.model.model.layers.30.self_attn.rotary_emb\n",
      "base_model.model.model.layers.30.mlp\n",
      "base_model.model.model.layers.30.mlp.gate_proj\n",
      "base_model.model.model.layers.30.mlp.up_proj\n",
      "base_model.model.model.layers.30.mlp.down_proj\n",
      "base_model.model.model.layers.30.mlp.act_fn\n",
      "base_model.model.model.layers.30.input_layernorm\n",
      "base_model.model.model.layers.30.post_attention_layernorm\n",
      "base_model.model.model.layers.31\n",
      "base_model.model.model.layers.31.self_attn\n",
      "base_model.model.model.layers.31.self_attn.q_proj\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_dropout\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_dropout.default\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_A\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_embedding_B\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.31.self_attn.k_proj\n",
      "base_model.model.model.layers.31.self_attn.v_proj\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_dropout\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_dropout.default\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_A\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_embedding_B\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_magnitude_vector\n",
      "base_model.model.model.layers.31.self_attn.o_proj\n",
      "base_model.model.model.layers.31.self_attn.rotary_emb\n",
      "base_model.model.model.layers.31.mlp\n",
      "base_model.model.model.layers.31.mlp.gate_proj\n",
      "base_model.model.model.layers.31.mlp.up_proj\n",
      "base_model.model.model.layers.31.mlp.down_proj\n",
      "base_model.model.model.layers.31.mlp.act_fn\n",
      "base_model.model.model.layers.31.input_layernorm\n",
      "base_model.model.model.layers.31.post_attention_layernorm\n",
      "base_model.model.model.norm\n",
      "base_model.model.model.rotary_emb\n",
      "base_model.model.lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fa7bc-471a-4160-a1be-13f7b57a527c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
